{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistilBert",
      "provenance": [],
      "authorship_tag": "ABX9TyOOFGkt3OtZCOw1lXXYmOZh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LennartKeller/PythonTrie/blob/master/DistilBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwpmQjtSSvhS",
        "colab_type": "code",
        "outputId": "fe3b2247-cebd-4c90-cf74-2f654e786e5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip uninstall --yes tensorflow\n",
        "!pip install tensorflow>=2.0.0\n",
        "\n",
        "!pip install transformers\n",
        "!pip install ktrain"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "\u001b[31mERROR: tensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.10.2 which is incompatible.\u001b[0m\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 17.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=df48fae3e440143ffb27290f7631c03d6a3f8889b17053dfe9d6fcb6bd7aa325\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n",
            "Collecting ktrain\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/3d/82a24d1b2ae3f963471ed2113b07faab7a240db1ce2e702d2f2b3beb2268/ktrain-0.8.3.tar.gz (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.1MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.21.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 19.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (3.1.2)\n",
            "Requirement already satisfied: pandas<1.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.25.3)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.2.2)\n",
            "Collecting keras_bert\n",
            "  Downloading https://files.pythonhosted.org/packages/df/fe/bf46de1ef9d1395cd735d8df5402f5d837ef82cfd348a252ad8f32feeaef/keras-bert-0.80.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.21.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.14.1)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 67.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.40)\n",
            "Collecting cchardet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/4e/847feebfc3e71c773b23ee06c74687b8c50a5a6d6aaff452a0a4f4eb9a32/cchardet-2.1.5-cp36-cp36m-manylinux1_x86_64.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 58.4MB/s \n",
            "\u001b[?25hCollecting networkx==2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/08/f20aef11d4c343b557e5de6b9548761811eb16e438cee3d32b1c66c8566b/networkx-2.3.zip (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 69.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.4.0)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from ktrain) (20.0)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.3.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.3.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ktrain) (5.5.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->ktrain) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->ktrain) (1.17.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas<1.0->ktrain) (2018.9)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_bert->ktrain) (2.2.5)\n",
            "Collecting keras-transformer>=0.30.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/57/496b1eab888171b0801a0a44d3245a7874b8d1cc04c1fbfdbb5e3327fc7a/keras-transformer-0.31.0.tar.gz\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->ktrain) (1.12.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx==2.3->ktrain) (4.4.1)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (6.2.2)\n",
            "Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (4.5.3)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (2.10.3)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (3.13)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (0.16.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (2.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (1.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (4.28.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (0.15.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (0.3.1.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (3.10.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (0.9.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (19.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (1.1.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (1.10.47)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (0.0.38)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (0.1.85)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (2.1.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.3.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.7.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (42.0.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (1.0.18)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert->ktrain) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert->ktrain) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert->ktrain) (1.1.0)\n",
            "Collecting keras-pos-embd>=0.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.22.0\n",
            "  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh->ktrain) (1.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets->ktrain) (1.6.0)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->ktrain) (1.13.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->ktrain) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->ktrain) (0.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->ktrain) (7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ktrain) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain) (0.1.8)\n",
            "Collecting keras-self-attention==0.41.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers->ktrain) (0.15.2)\n",
            "Building wheels for collected packages: ktrain, keras-bert, langdetect, networkx, seqeval, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.8.3-cp36-none-any.whl size=121601 sha256=1e725dd305ad4c1a30523eca0b948487ec3f50cc67a317a93a0059d5d966a0e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/e7/4b/6f839c9443003c6fabfc286d29228050d8e2e9b0d1d50399d0\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.80.0-cp36-none-any.whl size=37923 sha256=3a14ff9d820e284aadd44df7fa95e64367ff9372889ba0a0987a629c9f1b1a08\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/dc/87/3260cb91f3aa32c0f85c5375429a30c8fd988bbb48f5ee21b0\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993460 sha256=3c688ecd59fae4f7d343894aabf70ec55991d53c37e6c3688b637245832c86d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for networkx: filename=networkx-2.3-py2.py3-none-any.whl size=1556408 sha256=eb936e2a994ae512819baf2f04c8624ec5f2f598dc1f0d87554d52462e8c5ed5\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/63/64/3699be2a9d0ccdb37c7f16329acf3863fd76eda58c39c737af\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=09931f1f11a0903f9e1330eaaf1d856b8a847d7dec09a3f928a00521509d7c30\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.31.0-cp36-none-any.whl size=13385 sha256=36a8f2b08ec4338869ba6d2c6451d5f15a3e090ad9871fffc4915b2c3498fc61\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/c5/9a/5a5130240be614a7a6fa786765d7692ae97f82601e2161bb56\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7553 sha256=303f08f78839ce849e6e67a48501a69cc0c6bc2f469431c38aaa1e66d67e3c46\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=a26ade8db199e1cdca75ec0c9b5e13a8561d9726db1403137af5597151b32804\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=ba6f6894dce7408f899d0164dfa7c2ff7d61fe13e0c0ae632b2eec0bd5dee7a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5624 sha256=ceede81f3e920a566a9ed2a30f62eb9bda642e5e1ccbe6a329f788a56ab53846\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=c72d97728a86121c5fa94d21a01baf4104328300c5b128e2aae388a83022a087\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17290 sha256=95583b38190f4469b0bdb45a60f445a6007da9bf9bcc0abccc9779e7240f38d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built ktrain keras-bert langdetect networkx seqeval keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scikit-learn, keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert, langdetect, cchardet, networkx, seqeval, ktrain\n",
            "  Found existing installation: scikit-learn 0.22.1\n",
            "    Uninstalling scikit-learn-0.22.1:\n",
            "      Successfully uninstalled scikit-learn-0.22.1\n",
            "  Found existing installation: networkx 2.4\n",
            "    Uninstalling networkx-2.4:\n",
            "      Successfully uninstalled networkx-2.4\n",
            "Successfully installed cchardet-2.1.5 keras-bert-0.80.0 keras-embed-sim-0.7.0 keras-layer-normalization-0.14.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.31.0 ktrain-0.8.3 langdetect-1.0.7 networkx-2.3 scikit-learn-0.21.3 seqeval-0.0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwMaSbvkSZzR",
        "colab_type": "code",
        "outputId": "63667957-3710-42fb-b571-43d3c389d44d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9VHuoulSpGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-d5XkcpSvKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/gdrive/My Drive/full_dataset.csv').dropna()\n",
        "df = df[df.genre != 'NEWS-P4']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hlb8CWHfYfJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df.genre)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DMXuXoRU-Y1",
        "colab_type": "code",
        "outputId": "2f6e840e-f189-4b05-8d3a-fbd011b38a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train = df[df.period == 'P1']\n",
        "\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.1, random_state=42)\n",
        "\n",
        "df_test = df[df.period != 'P1']\n",
        "\n",
        "print(df_train.shape, df_val.shape, df_test.shape)\n",
        "\n",
        "train_data = transformer.preprocess_train(df_train.text, df_train.label)\n",
        "val_data = transformer.preprocess_test(df_val.text, df_val.label)\n",
        "test_data = transformer.preprocess_test(df_test.text, df_test.label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessing train...\n",
            "language: en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "preprocessing test...\n",
            "language: en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "preprocessing test...\n",
            "language: en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ4aif7FTZsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_NAME = 'bert-base-german-cased' # this modell does not work well (60% F1-Score trained in P1 (lemmas) and predicting the rest)\n",
        "#MODEL_NAME = 'xlm-mlm-ende-1024' # this modell works even worse (40% F1-Score with heavy overfitting)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLJuQeRqUmCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ktrain\n",
        "from ktrain import text\n",
        "transformer = text.Transformer(MODEL_NAME, maxlen=512, classes=le.classes_.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-fDNL42XOoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = transformer.get_classifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxSPd9UbawYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner = ktrain.get_learner(model, train_data=train_data, val_data=val_data, batch_size=4) #val_data=test_data,"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMBhEdQEZ0AA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#learner.lr_find(show_plot=True, max_epochs=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGw6SOz1a5dI",
        "colab_type": "code",
        "outputId": "1be99853-1e81-4fd3-eb46-85cd54157de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "learner.autofit(2e-4, checkpoint_folder='/tmp/saved_weights') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "early_stopping automatically enabled at patience=5\n",
            "reduce_on_plateau automatically enabled at patience=2\n",
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 0.0002...\n",
            "Train for 26 steps, validate for 3 steps\n",
            "Epoch 1/1024\n",
            "26/26 [==============================] - 32s 1s/step - loss: 2.0105 - accuracy: 0.1845 - val_loss: 1.8255 - val_accuracy: 0.2500\n",
            "Epoch 2/1024\n",
            "26/26 [==============================] - 17s 659ms/step - loss: 1.5672 - accuracy: 0.3689 - val_loss: 1.5991 - val_accuracy: 0.3333\n",
            "Epoch 3/1024\n",
            "26/26 [==============================] - 17s 648ms/step - loss: 1.5359 - accuracy: 0.2913 - val_loss: 1.4264 - val_accuracy: 0.4167\n",
            "Epoch 4/1024\n",
            "26/26 [==============================] - 16s 614ms/step - loss: 1.5392 - accuracy: 0.3204 - val_loss: 1.4298 - val_accuracy: 0.3333\n",
            "Epoch 5/1024\n",
            "26/26 [==============================] - 17s 643ms/step - loss: 1.4938 - accuracy: 0.3495 - val_loss: 1.3016 - val_accuracy: 0.4167\n",
            "Epoch 6/1024\n",
            "26/26 [==============================] - 16s 622ms/step - loss: 1.4921 - accuracy: 0.3107 - val_loss: 1.3905 - val_accuracy: 0.5000\n",
            "Epoch 7/1024\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 1.4792 - accuracy: 0.3100\n",
            "Epoch 00007: Reducing Max LR on Plateau: new max lr will be 0.0001 (if not early_stopping).\n",
            "26/26 [==============================] - 16s 620ms/step - loss: 1.4683 - accuracy: 0.3204 - val_loss: 1.3597 - val_accuracy: 0.2500\n",
            "Epoch 8/1024\n",
            "26/26 [==============================] - 16s 617ms/step - loss: 1.4402 - accuracy: 0.3301 - val_loss: 1.4044 - val_accuracy: 0.3333\n",
            "Epoch 9/1024\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 1.4446 - accuracy: 0.3000\n",
            "Epoch 00009: Reducing Max LR on Plateau: new max lr will be 5e-05 (if not early_stopping).\n",
            "26/26 [==============================] - 16s 619ms/step - loss: 1.4618 - accuracy: 0.2913 - val_loss: 1.3409 - val_accuracy: 0.4167\n",
            "Epoch 10/1024\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 1.4538 - accuracy: 0.3200Restoring model weights from the end of the best epoch.\n",
            "26/26 [==============================] - 16s 629ms/step - loss: 1.4388 - accuracy: 0.3301 - val_loss: 1.4094 - val_accuracy: 0.3333\n",
            "Epoch 00010: early stopping\n",
            "Weights from best epoch have been loaded into model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe2b0fc73c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1gbzYP1bOBy",
        "colab_type": "code",
        "outputId": "05a2ead0-4756-4039-e69d-8e59e8c726c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "source": [
        "learner.validate(class_names=transformer.get_classes())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-37cbe6a5d3f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, val_data, print_report, class_names)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprint_report\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m                 \u001b[0;34m\"parameter\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1877\u001b[0m             )\n\u001b[1;32m   1878\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of classes, 6, does not match size of target_names, 7. Try specifying the labels parameter"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYpijYPLbo_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm -rf /tmp/saved_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUhvrKyk1DXa",
        "colab_type": "text"
      },
      "source": [
        "# Try ktrains other models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqI2wZ3Lp5C0",
        "colab_type": "code",
        "outputId": "abbde51c-e7c3-45b3-c73a-c14f7d71659b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "text.print_text_classifiers()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fasttext: a fastText-like model [http://arxiv.org/pdf/1607.01759.pdf]\n",
            "logreg: logistic regression using a trainable Embedding layer\n",
            "nbsvm: NBSVM model [http://www.aclweb.org/anthology/P12-2018]\n",
            "bigru: Bidirectional GRU with pretrained word vectors [https://arxiv.org/abs/1712.09405]\n",
            "standard_gru: simple 2-layer GRU with randomly initialized embeddings\n",
            "bert: Bidirectional Encoder Representations from Transformers (BERT) [https://arxiv.org/abs/1810.04805]\n",
            "distilbert: distilled, smaller, and faster BERT from Hugging Face [https://arxiv.org/abs/1910.01108]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0JvHKgYztBP",
        "colab_type": "code",
        "outputId": "41fdaadc-1ef3-4c5e-c809-178a7c5387ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_array(x_train=df_train.text,\n",
        "                                                                      y_train=df_train.label,\n",
        "                                                                      x_test=df_test.text,\n",
        "                                                                      y_test=df_test.label,\n",
        "                                                                      lang='de')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "language: de\n",
            "Word Counts: 30574\n",
            "Nrows: 115\n",
            "115 train sequences\n",
            "Average train sequence length: 2184\n",
            "x_train shape: (115,400)\n",
            "y_train shape: (115,8)\n",
            "859 test sequences\n",
            "Average test sequence length: 1267\n",
            "x_test shape: (859,400)\n",
            "y_test shape: (859,8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml7LfNvA1GD4",
        "colab_type": "text"
      },
      "source": [
        "## 1. BiGru"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ky38kDD0qzj",
        "colab_type": "code",
        "outputId": "6da2011e-9797-4b52-ad7a-b07cd4298970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model = text.text_classifier('bigru', (x_train, y_train), preproc=preproc)\n",
        "learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "compiling word ID features...\n",
            "maxlen is 400\n",
            "processing pretrained word vectors...\n",
            "Loading pretrained word vectors...this may take a few moments...\n",
            "Done.\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbjU2f6t00Ss",
        "colab_type": "code",
        "outputId": "c74c5554-401b-4c66-8288-c908176d6738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "learner.autofit(2e-5, 20) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 2e-05...\n",
            "Train on 115 samples, validate on 859 samples\n",
            "Epoch 1/20\n",
            "115/115 [==============================] - 9s 81ms/sample - loss: 2.2583 - accuracy: 0.1217 - val_loss: 2.2458 - val_accuracy: 0.1106\n",
            "Epoch 2/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.2606 - accuracy: 0.1130 - val_loss: 2.2393 - val_accuracy: 0.1106\n",
            "Epoch 3/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.2449 - accuracy: 0.1391 - val_loss: 2.2327 - val_accuracy: 0.1106\n",
            "Epoch 4/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.2442 - accuracy: 0.1217 - val_loss: 2.2261 - val_accuracy: 0.1106\n",
            "Epoch 5/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.2405 - accuracy: 0.1217 - val_loss: 2.2198 - val_accuracy: 0.1106\n",
            "Epoch 6/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.2316 - accuracy: 0.1304 - val_loss: 2.2138 - val_accuracy: 0.1106\n",
            "Epoch 7/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.2361 - accuracy: 0.1130 - val_loss: 2.2078 - val_accuracy: 0.1106\n",
            "Epoch 8/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.2202 - accuracy: 0.1217 - val_loss: 2.2020 - val_accuracy: 0.1106\n",
            "Epoch 9/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.2128 - accuracy: 0.1304 - val_loss: 2.1962 - val_accuracy: 0.1106\n",
            "Epoch 10/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.1975 - accuracy: 0.1391 - val_loss: 2.1905 - val_accuracy: 0.1106\n",
            "Epoch 11/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.2037 - accuracy: 0.1217 - val_loss: 2.1847 - val_accuracy: 0.1106\n",
            "Epoch 12/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.1983 - accuracy: 0.1304 - val_loss: 2.1789 - val_accuracy: 0.1106\n",
            "Epoch 13/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.1958 - accuracy: 0.1304 - val_loss: 2.1733 - val_accuracy: 0.1106\n",
            "Epoch 14/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.1883 - accuracy: 0.1217 - val_loss: 2.1679 - val_accuracy: 0.1106\n",
            "Epoch 15/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.1912 - accuracy: 0.1304 - val_loss: 2.1626 - val_accuracy: 0.1106\n",
            "Epoch 16/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.1693 - accuracy: 0.1391 - val_loss: 2.1576 - val_accuracy: 0.1106\n",
            "Epoch 17/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.1679 - accuracy: 0.1478 - val_loss: 2.1525 - val_accuracy: 0.1106\n",
            "Epoch 18/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.1698 - accuracy: 0.1565 - val_loss: 2.1477 - val_accuracy: 0.1106\n",
            "Epoch 19/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.1682 - accuracy: 0.1304 - val_loss: 2.1430 - val_accuracy: 0.1106\n",
            "Epoch 20/20\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.1523 - accuracy: 0.1478 - val_loss: 2.1383 - val_accuracy: 0.1106\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbe240122b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogp1Uulm1WRX",
        "colab_type": "code",
        "outputId": "9733ad27-d3cf-414c-b37d-837714d2ba50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "learner.validate(class_names=transformer.get_classes())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DRAM       0.00      0.00      0.00        89\n",
            "        HUMA       0.00      0.00      0.00        97\n",
            "        LEGA       0.00      0.00      0.00        97\n",
            "        NARR       0.00      0.00      0.00        97\n",
            "        NEWS       0.00      0.00      0.00       286\n",
            "     NEWS-P4       0.00      0.00      0.00         1\n",
            "        SCIE       0.00      0.00      0.00        97\n",
            "        SERM       0.11      1.00      0.20        95\n",
            "\n",
            "    accuracy                           0.11       859\n",
            "   macro avg       0.01      0.12      0.02       859\n",
            "weighted avg       0.01      0.11      0.02       859\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,  89],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  97],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  97],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  97],\n",
              "       [  0,   0,   0,   0,   0,   1,   0, 285],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   1],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  97],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  95]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3wpxFq21Mk6",
        "colab_type": "text"
      },
      "source": [
        "## 2. FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQqOIqti1AxW",
        "colab_type": "code",
        "outputId": "3315a6c6-ea19-4195-af5b-6dacc24b571c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "model = text.text_classifier('fasttext', (x_train, y_train), preproc=preproc)\n",
        "learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "compiling word ID features...\n",
            "maxlen is 400\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM5b3q901Rf-",
        "colab_type": "code",
        "outputId": "bcfa9d92-937c-4614-9e14-c8a65fea3a41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learner.autofit(0.001, 200) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 0.001...\n",
            "Train on 115 samples, validate on 859 samples\n",
            "Epoch 1/200\n",
            "115/115 [==============================] - 1s 7ms/sample - loss: 2.7666 - accuracy: 0.1043 - val_loss: 2.0508 - val_accuracy: 0.1106\n",
            "Epoch 2/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 2.6106 - accuracy: 0.1565 - val_loss: 2.0473 - val_accuracy: 0.1106\n",
            "Epoch 3/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 2.5861 - accuracy: 0.1826 - val_loss: 2.0446 - val_accuracy: 0.1106\n",
            "Epoch 4/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 2.6786 - accuracy: 0.0783 - val_loss: 2.0425 - val_accuracy: 0.1769\n",
            "Epoch 5/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 2.4868 - accuracy: 0.1739 - val_loss: 2.0407 - val_accuracy: 0.3329\n",
            "Epoch 6/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 2.3828 - accuracy: 0.1565 - val_loss: 2.0385 - val_accuracy: 0.3329\n",
            "Epoch 7/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 2.4508 - accuracy: 0.1217 - val_loss: 2.0359 - val_accuracy: 0.3329\n",
            "Epoch 8/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 2.4663 - accuracy: 0.1826 - val_loss: 2.0333 - val_accuracy: 0.3329\n",
            "Epoch 9/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 2.3513 - accuracy: 0.2348 - val_loss: 2.0307 - val_accuracy: 0.3329\n",
            "Epoch 10/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 2.4032 - accuracy: 0.1565 - val_loss: 2.0284 - val_accuracy: 0.3329\n",
            "Epoch 11/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 2.3314 - accuracy: 0.1913 - val_loss: 2.0263 - val_accuracy: 0.3329\n",
            "Epoch 12/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 2.3382 - accuracy: 0.2174 - val_loss: 2.0248 - val_accuracy: 0.3329\n",
            "Epoch 13/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 2.0318 - accuracy: 0.2783 - val_loss: 2.0235 - val_accuracy: 0.3329\n",
            "Epoch 14/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 2.1848 - accuracy: 0.2174 - val_loss: 2.0226 - val_accuracy: 0.3329\n",
            "Epoch 15/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.9968 - accuracy: 0.2957 - val_loss: 2.0214 - val_accuracy: 0.3329\n",
            "Epoch 16/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 2.0253 - accuracy: 0.2783 - val_loss: 2.0197 - val_accuracy: 0.3329\n",
            "Epoch 17/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.9709 - accuracy: 0.2609 - val_loss: 2.0183 - val_accuracy: 0.3329\n",
            "Epoch 18/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.9265 - accuracy: 0.2957 - val_loss: 2.0165 - val_accuracy: 0.3329\n",
            "Epoch 19/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.8489 - accuracy: 0.3304 - val_loss: 2.0143 - val_accuracy: 0.3329\n",
            "Epoch 20/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.9055 - accuracy: 0.3217 - val_loss: 2.0122 - val_accuracy: 0.3329\n",
            "Epoch 21/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.9906 - accuracy: 0.2870 - val_loss: 2.0101 - val_accuracy: 0.3329\n",
            "Epoch 22/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.8966 - accuracy: 0.2870 - val_loss: 2.0085 - val_accuracy: 0.3329\n",
            "Epoch 23/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.9381 - accuracy: 0.2783 - val_loss: 2.0073 - val_accuracy: 0.3329\n",
            "Epoch 24/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.7428 - accuracy: 0.3652 - val_loss: 2.0058 - val_accuracy: 0.3329\n",
            "Epoch 25/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.7523 - accuracy: 0.4087 - val_loss: 2.0048 - val_accuracy: 0.3329\n",
            "Epoch 26/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.8361 - accuracy: 0.3391 - val_loss: 2.0038 - val_accuracy: 0.3329\n",
            "Epoch 27/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.8551 - accuracy: 0.2522 - val_loss: 2.0023 - val_accuracy: 0.3329\n",
            "Epoch 28/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.6912 - accuracy: 0.3913 - val_loss: 2.0006 - val_accuracy: 0.3329\n",
            "Epoch 29/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.7768 - accuracy: 0.2783 - val_loss: 1.9992 - val_accuracy: 0.3329\n",
            "Epoch 30/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.7316 - accuracy: 0.3652 - val_loss: 1.9978 - val_accuracy: 0.3329\n",
            "Epoch 31/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.7164 - accuracy: 0.3913 - val_loss: 1.9967 - val_accuracy: 0.3329\n",
            "Epoch 32/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.6200 - accuracy: 0.4174 - val_loss: 1.9955 - val_accuracy: 0.3329\n",
            "Epoch 33/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.5433 - accuracy: 0.4000 - val_loss: 1.9946 - val_accuracy: 0.3329\n",
            "Epoch 34/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.5172 - accuracy: 0.4522 - val_loss: 1.9936 - val_accuracy: 0.3329\n",
            "Epoch 35/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.7192 - accuracy: 0.3652 - val_loss: 1.9923 - val_accuracy: 0.3329\n",
            "Epoch 36/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.5694 - accuracy: 0.3826 - val_loss: 1.9909 - val_accuracy: 0.3329\n",
            "Epoch 37/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.4211 - accuracy: 0.4870 - val_loss: 1.9890 - val_accuracy: 0.3329\n",
            "Epoch 38/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.5340 - accuracy: 0.4609 - val_loss: 1.9872 - val_accuracy: 0.3329\n",
            "Epoch 39/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.4694 - accuracy: 0.4348 - val_loss: 1.9853 - val_accuracy: 0.3329\n",
            "Epoch 40/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.4337 - accuracy: 0.4609 - val_loss: 1.9834 - val_accuracy: 0.3353\n",
            "Epoch 41/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.5478 - accuracy: 0.4000 - val_loss: 1.9807 - val_accuracy: 0.3364\n",
            "Epoch 42/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.3570 - accuracy: 0.5391 - val_loss: 1.9783 - val_accuracy: 0.3376\n",
            "Epoch 43/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.3664 - accuracy: 0.5391 - val_loss: 1.9761 - val_accuracy: 0.3376\n",
            "Epoch 44/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.3752 - accuracy: 0.5478 - val_loss: 1.9738 - val_accuracy: 0.3376\n",
            "Epoch 45/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.3839 - accuracy: 0.4957 - val_loss: 1.9714 - val_accuracy: 0.3376\n",
            "Epoch 46/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.2638 - accuracy: 0.5304 - val_loss: 1.9691 - val_accuracy: 0.3388\n",
            "Epoch 47/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.3236 - accuracy: 0.5391 - val_loss: 1.9665 - val_accuracy: 0.3388\n",
            "Epoch 48/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.1948 - accuracy: 0.5826 - val_loss: 1.9645 - val_accuracy: 0.3399\n",
            "Epoch 49/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.2603 - accuracy: 0.5304 - val_loss: 1.9621 - val_accuracy: 0.3423\n",
            "Epoch 50/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.2896 - accuracy: 0.5217 - val_loss: 1.9600 - val_accuracy: 0.3481\n",
            "Epoch 51/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.1431 - accuracy: 0.5652 - val_loss: 1.9577 - val_accuracy: 0.3492\n",
            "Epoch 52/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.2157 - accuracy: 0.5565 - val_loss: 1.9559 - val_accuracy: 0.3551\n",
            "Epoch 53/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.2676 - accuracy: 0.5391 - val_loss: 1.9543 - val_accuracy: 0.3632\n",
            "Epoch 54/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.1840 - accuracy: 0.6000 - val_loss: 1.9522 - val_accuracy: 0.3644\n",
            "Epoch 55/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.0371 - accuracy: 0.6783 - val_loss: 1.9508 - val_accuracy: 0.3679\n",
            "Epoch 56/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.1819 - accuracy: 0.5826 - val_loss: 1.9491 - val_accuracy: 0.3737\n",
            "Epoch 57/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.1893 - accuracy: 0.6087 - val_loss: 1.9475 - val_accuracy: 0.3737\n",
            "Epoch 58/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.1981 - accuracy: 0.5478 - val_loss: 1.9459 - val_accuracy: 0.3783\n",
            "Epoch 59/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.2202 - accuracy: 0.5826 - val_loss: 1.9435 - val_accuracy: 0.3818\n",
            "Epoch 60/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.1536 - accuracy: 0.5739 - val_loss: 1.9412 - val_accuracy: 0.3877\n",
            "Epoch 61/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.9204 - accuracy: 0.7217 - val_loss: 1.9381 - val_accuracy: 0.3900\n",
            "Epoch 62/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 1.1027 - accuracy: 0.6696 - val_loss: 1.9353 - val_accuracy: 0.3900\n",
            "Epoch 63/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.0552 - accuracy: 0.6087 - val_loss: 1.9329 - val_accuracy: 0.3888\n",
            "Epoch 64/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.9804 - accuracy: 0.6696 - val_loss: 1.9306 - val_accuracy: 0.3958\n",
            "Epoch 65/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.8459 - accuracy: 0.7304 - val_loss: 1.9277 - val_accuracy: 0.3981\n",
            "Epoch 66/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.9351 - accuracy: 0.6783 - val_loss: 1.9248 - val_accuracy: 0.3981\n",
            "Epoch 67/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.9114 - accuracy: 0.6522 - val_loss: 1.9217 - val_accuracy: 0.4016\n",
            "Epoch 68/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.9049 - accuracy: 0.7217 - val_loss: 1.9184 - val_accuracy: 0.4051\n",
            "Epoch 69/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.7779 - accuracy: 0.7304 - val_loss: 1.9143 - val_accuracy: 0.4086\n",
            "Epoch 70/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.9098 - accuracy: 0.7130 - val_loss: 1.9109 - val_accuracy: 0.4075\n",
            "Epoch 71/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.8402 - accuracy: 0.7043 - val_loss: 1.9076 - val_accuracy: 0.4063\n",
            "Epoch 72/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.9646 - accuracy: 0.6957 - val_loss: 1.9050 - val_accuracy: 0.4098\n",
            "Epoch 73/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.9211 - accuracy: 0.7217 - val_loss: 1.9023 - val_accuracy: 0.4075\n",
            "Epoch 74/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 1.0247 - accuracy: 0.6435 - val_loss: 1.8995 - val_accuracy: 0.4144\n",
            "Epoch 75/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.8256 - accuracy: 0.7130 - val_loss: 1.8977 - val_accuracy: 0.4086\n",
            "Epoch 76/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.8629 - accuracy: 0.6957 - val_loss: 1.8946 - val_accuracy: 0.4109\n",
            "Epoch 77/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.7115 - accuracy: 0.7478 - val_loss: 1.8923 - val_accuracy: 0.4109\n",
            "Epoch 78/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.9019 - accuracy: 0.6870 - val_loss: 1.8885 - val_accuracy: 0.4121\n",
            "Epoch 79/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.7403 - accuracy: 0.7652 - val_loss: 1.8862 - val_accuracy: 0.4109\n",
            "Epoch 80/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.8110 - accuracy: 0.7217 - val_loss: 1.8829 - val_accuracy: 0.4098\n",
            "Epoch 81/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.8047 - accuracy: 0.6957 - val_loss: 1.8797 - val_accuracy: 0.4098\n",
            "Epoch 82/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.6816 - accuracy: 0.7652 - val_loss: 1.8771 - val_accuracy: 0.4040\n",
            "Epoch 83/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.7594 - accuracy: 0.7739 - val_loss: 1.8749 - val_accuracy: 0.4063\n",
            "Epoch 84/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.8670 - accuracy: 0.6696 - val_loss: 1.8722 - val_accuracy: 0.4051\n",
            "Epoch 85/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.8551 - accuracy: 0.6783 - val_loss: 1.8689 - val_accuracy: 0.4051\n",
            "Epoch 86/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.7835 - accuracy: 0.7217 - val_loss: 1.8660 - val_accuracy: 0.4005\n",
            "Epoch 87/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.7006 - accuracy: 0.7652 - val_loss: 1.8621 - val_accuracy: 0.4016\n",
            "Epoch 88/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.6637 - accuracy: 0.8174 - val_loss: 1.8590 - val_accuracy: 0.3970\n",
            "Epoch 89/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.8341 - accuracy: 0.7043 - val_loss: 1.8555 - val_accuracy: 0.3935\n",
            "Epoch 90/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.6639 - accuracy: 0.7913 - val_loss: 1.8524 - val_accuracy: 0.3923\n",
            "Epoch 91/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.7626 - accuracy: 0.7565 - val_loss: 1.8488 - val_accuracy: 0.3935\n",
            "Epoch 92/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.6781 - accuracy: 0.7913 - val_loss: 1.8445 - val_accuracy: 0.3946\n",
            "Epoch 93/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.5399 - accuracy: 0.8435 - val_loss: 1.8411 - val_accuracy: 0.3923\n",
            "Epoch 94/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.7663 - accuracy: 0.7478 - val_loss: 1.8378 - val_accuracy: 0.3888\n",
            "Epoch 95/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.5956 - accuracy: 0.7826 - val_loss: 1.8354 - val_accuracy: 0.3853\n",
            "Epoch 96/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.6729 - accuracy: 0.7652 - val_loss: 1.8317 - val_accuracy: 0.3830\n",
            "Epoch 97/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.7004 - accuracy: 0.7565 - val_loss: 1.8286 - val_accuracy: 0.3795\n",
            "Epoch 98/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.6353 - accuracy: 0.8000 - val_loss: 1.8245 - val_accuracy: 0.3783\n",
            "Epoch 99/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.5440 - accuracy: 0.8522 - val_loss: 1.8206 - val_accuracy: 0.3818\n",
            "Epoch 100/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.6120 - accuracy: 0.8087 - val_loss: 1.8175 - val_accuracy: 0.3853\n",
            "Epoch 101/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.5653 - accuracy: 0.8000 - val_loss: 1.8141 - val_accuracy: 0.3865\n",
            "Epoch 102/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.5325 - accuracy: 0.8435 - val_loss: 1.8116 - val_accuracy: 0.3865\n",
            "Epoch 103/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.4648 - accuracy: 0.8609 - val_loss: 1.8091 - val_accuracy: 0.3842\n",
            "Epoch 104/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.6571 - accuracy: 0.7652 - val_loss: 1.8065 - val_accuracy: 0.3807\n",
            "Epoch 105/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.5492 - accuracy: 0.8348 - val_loss: 1.8039 - val_accuracy: 0.3783\n",
            "Epoch 106/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.5976 - accuracy: 0.8174 - val_loss: 1.8010 - val_accuracy: 0.3783\n",
            "Epoch 107/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.6058 - accuracy: 0.8087 - val_loss: 1.7984 - val_accuracy: 0.3749\n",
            "Epoch 108/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.5238 - accuracy: 0.8261 - val_loss: 1.7955 - val_accuracy: 0.3760\n",
            "Epoch 109/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.6403 - accuracy: 0.7739 - val_loss: 1.7918 - val_accuracy: 0.3772\n",
            "Epoch 110/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.4606 - accuracy: 0.8348 - val_loss: 1.7898 - val_accuracy: 0.3725\n",
            "Epoch 111/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.4785 - accuracy: 0.8435 - val_loss: 1.7865 - val_accuracy: 0.3714\n",
            "Epoch 112/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.4746 - accuracy: 0.8435 - val_loss: 1.7835 - val_accuracy: 0.3737\n",
            "Epoch 113/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.5606 - accuracy: 0.7913 - val_loss: 1.7805 - val_accuracy: 0.3725\n",
            "Epoch 114/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.5410 - accuracy: 0.8087 - val_loss: 1.7777 - val_accuracy: 0.3725\n",
            "Epoch 115/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.3677 - accuracy: 0.9304 - val_loss: 1.7751 - val_accuracy: 0.3702\n",
            "Epoch 116/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.5124 - accuracy: 0.8000 - val_loss: 1.7722 - val_accuracy: 0.3690\n",
            "Epoch 117/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.4261 - accuracy: 0.8783 - val_loss: 1.7705 - val_accuracy: 0.3679\n",
            "Epoch 118/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.4844 - accuracy: 0.8261 - val_loss: 1.7681 - val_accuracy: 0.3679\n",
            "Epoch 119/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.4288 - accuracy: 0.8957 - val_loss: 1.7656 - val_accuracy: 0.3632\n",
            "Epoch 120/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.4567 - accuracy: 0.8609 - val_loss: 1.7630 - val_accuracy: 0.3667\n",
            "Epoch 121/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.4116 - accuracy: 0.8609 - val_loss: 1.7613 - val_accuracy: 0.3667\n",
            "Epoch 122/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.3919 - accuracy: 0.8957 - val_loss: 1.7591 - val_accuracy: 0.3655\n",
            "Epoch 123/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.4595 - accuracy: 0.8609 - val_loss: 1.7584 - val_accuracy: 0.3620\n",
            "Epoch 124/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.4365 - accuracy: 0.8696 - val_loss: 1.7586 - val_accuracy: 0.3644\n",
            "Epoch 125/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.4652 - accuracy: 0.8522 - val_loss: 1.7597 - val_accuracy: 0.3702\n",
            "Epoch 126/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.4891 - accuracy: 0.8348 - val_loss: 1.7581 - val_accuracy: 0.3702\n",
            "Epoch 127/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.4849 - accuracy: 0.8000 - val_loss: 1.7569 - val_accuracy: 0.3714\n",
            "Epoch 128/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.3889 - accuracy: 0.8609 - val_loss: 1.7563 - val_accuracy: 0.3702\n",
            "Epoch 129/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.4176 - accuracy: 0.8870 - val_loss: 1.7562 - val_accuracy: 0.3679\n",
            "Epoch 130/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2845 - accuracy: 0.9217 - val_loss: 1.7550 - val_accuracy: 0.3679\n",
            "Epoch 131/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.3628 - accuracy: 0.8696 - val_loss: 1.7522 - val_accuracy: 0.3667\n",
            "Epoch 132/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.3453 - accuracy: 0.8870 - val_loss: 1.7497 - val_accuracy: 0.3644\n",
            "Epoch 133/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.3879 - accuracy: 0.9130 - val_loss: 1.7484 - val_accuracy: 0.3597\n",
            "Epoch 134/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.4236 - accuracy: 0.9304 - val_loss: 1.7474 - val_accuracy: 0.3574\n",
            "Epoch 135/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2741 - accuracy: 0.9217 - val_loss: 1.7471 - val_accuracy: 0.3586\n",
            "Epoch 136/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.3161 - accuracy: 0.8783 - val_loss: 1.7468 - val_accuracy: 0.3597\n",
            "Epoch 137/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.3659 - accuracy: 0.8783 - val_loss: 1.7463 - val_accuracy: 0.3620\n",
            "Epoch 138/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2906 - accuracy: 0.9304 - val_loss: 1.7467 - val_accuracy: 0.3574\n",
            "Epoch 139/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2654 - accuracy: 0.9565 - val_loss: 1.7479 - val_accuracy: 0.3562\n",
            "Epoch 140/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.3581 - accuracy: 0.9043 - val_loss: 1.7485 - val_accuracy: 0.3539\n",
            "Epoch 141/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.3308 - accuracy: 0.8783 - val_loss: 1.7498 - val_accuracy: 0.3516\n",
            "Epoch 142/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.3373 - accuracy: 0.8870 - val_loss: 1.7495 - val_accuracy: 0.3469\n",
            "Epoch 143/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.4243 - accuracy: 0.8783 - val_loss: 1.7483 - val_accuracy: 0.3527\n",
            "Epoch 144/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.3555 - accuracy: 0.8957 - val_loss: 1.7473 - val_accuracy: 0.3539\n",
            "Epoch 145/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.3877 - accuracy: 0.8696 - val_loss: 1.7487 - val_accuracy: 0.3492\n",
            "Epoch 146/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.4312 - accuracy: 0.8609 - val_loss: 1.7489 - val_accuracy: 0.3492\n",
            "Epoch 147/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2832 - accuracy: 0.9043 - val_loss: 1.7513 - val_accuracy: 0.3469\n",
            "Epoch 148/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.3655 - accuracy: 0.8696 - val_loss: 1.7530 - val_accuracy: 0.3481\n",
            "Epoch 149/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.4221 - accuracy: 0.8348 - val_loss: 1.7551 - val_accuracy: 0.3446\n",
            "Epoch 150/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.3189 - accuracy: 0.9217 - val_loss: 1.7553 - val_accuracy: 0.3446\n",
            "Epoch 151/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.4331 - accuracy: 0.8261 - val_loss: 1.7567 - val_accuracy: 0.3446\n",
            "Epoch 152/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.4567 - accuracy: 0.8522 - val_loss: 1.7576 - val_accuracy: 0.3446\n",
            "Epoch 153/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.3384 - accuracy: 0.9130 - val_loss: 1.7586 - val_accuracy: 0.3446\n",
            "Epoch 154/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.3084 - accuracy: 0.9130 - val_loss: 1.7591 - val_accuracy: 0.3458\n",
            "Epoch 155/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2525 - accuracy: 0.9043 - val_loss: 1.7583 - val_accuracy: 0.3481\n",
            "Epoch 156/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2545 - accuracy: 0.9304 - val_loss: 1.7590 - val_accuracy: 0.3492\n",
            "Epoch 157/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2796 - accuracy: 0.8957 - val_loss: 1.7597 - val_accuracy: 0.3527\n",
            "Epoch 158/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2890 - accuracy: 0.9304 - val_loss: 1.7605 - val_accuracy: 0.3527\n",
            "Epoch 159/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.3361 - accuracy: 0.8957 - val_loss: 1.7613 - val_accuracy: 0.3527\n",
            "Epoch 160/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2643 - accuracy: 0.8957 - val_loss: 1.7624 - val_accuracy: 0.3516\n",
            "Epoch 161/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.3769 - accuracy: 0.8957 - val_loss: 1.7627 - val_accuracy: 0.3504\n",
            "Epoch 162/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2936 - accuracy: 0.8696 - val_loss: 1.7631 - val_accuracy: 0.3481\n",
            "Epoch 163/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2589 - accuracy: 0.9130 - val_loss: 1.7641 - val_accuracy: 0.3504\n",
            "Epoch 164/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2160 - accuracy: 0.9391 - val_loss: 1.7657 - val_accuracy: 0.3527\n",
            "Epoch 165/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2005 - accuracy: 0.9478 - val_loss: 1.7672 - val_accuracy: 0.3516\n",
            "Epoch 166/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.3159 - accuracy: 0.9217 - val_loss: 1.7688 - val_accuracy: 0.3504\n",
            "Epoch 167/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2937 - accuracy: 0.9130 - val_loss: 1.7697 - val_accuracy: 0.3492\n",
            "Epoch 168/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2376 - accuracy: 0.9217 - val_loss: 1.7718 - val_accuracy: 0.3492\n",
            "Epoch 169/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.3084 - accuracy: 0.9043 - val_loss: 1.7734 - val_accuracy: 0.3469\n",
            "Epoch 170/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.3205 - accuracy: 0.8870 - val_loss: 1.7757 - val_accuracy: 0.3481\n",
            "Epoch 171/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2436 - accuracy: 0.9478 - val_loss: 1.7789 - val_accuracy: 0.3446\n",
            "Epoch 172/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2367 - accuracy: 0.9478 - val_loss: 1.7807 - val_accuracy: 0.3411\n",
            "Epoch 173/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.1955 - accuracy: 0.9478 - val_loss: 1.7829 - val_accuracy: 0.3423\n",
            "Epoch 174/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2506 - accuracy: 0.9217 - val_loss: 1.7835 - val_accuracy: 0.3411\n",
            "Epoch 175/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2510 - accuracy: 0.9478 - val_loss: 1.7849 - val_accuracy: 0.3411\n",
            "Epoch 176/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2327 - accuracy: 0.9130 - val_loss: 1.7877 - val_accuracy: 0.3411\n",
            "Epoch 177/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2472 - accuracy: 0.9043 - val_loss: 1.7893 - val_accuracy: 0.3399\n",
            "Epoch 178/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2702 - accuracy: 0.9130 - val_loss: 1.7914 - val_accuracy: 0.3399\n",
            "Epoch 179/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2469 - accuracy: 0.9217 - val_loss: 1.7920 - val_accuracy: 0.3411\n",
            "Epoch 180/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.1900 - accuracy: 0.9739 - val_loss: 1.7935 - val_accuracy: 0.3399\n",
            "Epoch 181/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.1724 - accuracy: 0.9652 - val_loss: 1.7951 - val_accuracy: 0.3446\n",
            "Epoch 182/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2567 - accuracy: 0.9391 - val_loss: 1.7958 - val_accuracy: 0.3458\n",
            "Epoch 183/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2527 - accuracy: 0.8957 - val_loss: 1.7970 - val_accuracy: 0.3434\n",
            "Epoch 184/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2647 - accuracy: 0.9043 - val_loss: 1.7974 - val_accuracy: 0.3434\n",
            "Epoch 185/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2240 - accuracy: 0.9478 - val_loss: 1.7989 - val_accuracy: 0.3423\n",
            "Epoch 186/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2084 - accuracy: 0.9565 - val_loss: 1.8002 - val_accuracy: 0.3423\n",
            "Epoch 187/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2351 - accuracy: 0.9043 - val_loss: 1.8021 - val_accuracy: 0.3446\n",
            "Epoch 188/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2477 - accuracy: 0.9217 - val_loss: 1.8032 - val_accuracy: 0.3434\n",
            "Epoch 189/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.1694 - accuracy: 0.9565 - val_loss: 1.8052 - val_accuracy: 0.3411\n",
            "Epoch 190/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.1529 - accuracy: 0.9739 - val_loss: 1.8063 - val_accuracy: 0.3446\n",
            "Epoch 191/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.1694 - accuracy: 0.9391 - val_loss: 1.8064 - val_accuracy: 0.3423\n",
            "Epoch 192/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.1884 - accuracy: 0.9391 - val_loss: 1.8085 - val_accuracy: 0.3411\n",
            "Epoch 193/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.1398 - accuracy: 0.9652 - val_loss: 1.8113 - val_accuracy: 0.3388\n",
            "Epoch 194/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.2348 - accuracy: 0.9043 - val_loss: 1.8136 - val_accuracy: 0.3388\n",
            "Epoch 195/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.1310 - accuracy: 0.9913 - val_loss: 1.8152 - val_accuracy: 0.3388\n",
            "Epoch 196/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.2085 - accuracy: 0.9391 - val_loss: 1.8168 - val_accuracy: 0.3388\n",
            "Epoch 197/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.1741 - accuracy: 0.9478 - val_loss: 1.8185 - val_accuracy: 0.3411\n",
            "Epoch 198/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.1646 - accuracy: 0.9565 - val_loss: 1.8212 - val_accuracy: 0.3388\n",
            "Epoch 199/200\n",
            "115/115 [==============================] - 0s 2ms/sample - loss: 0.1795 - accuracy: 0.9565 - val_loss: 1.8228 - val_accuracy: 0.3388\n",
            "Epoch 200/200\n",
            "115/115 [==============================] - 0s 1ms/sample - loss: 0.1547 - accuracy: 0.9652 - val_loss: 1.8244 - val_accuracy: 0.3388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc03895f080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qNL6bOD1W6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.validate(class_names=transformer.get_classes())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avBWN3YE2-Z9",
        "colab_type": "text"
      },
      "source": [
        "## 3. LogReg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp4TfGE42_l9",
        "colab_type": "code",
        "outputId": "b007ada8-cb62-4389-ba3c-f1e633e1adf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "model = text.text_classifier('logreg', (x_train, y_train), preproc=preproc)\n",
        "learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "compiling word ID features...\n",
            "maxlen is 400\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0orYznm3m5e",
        "colab_type": "code",
        "outputId": "ee253259-a9b4-4d1f-aff7-3354b5a8674c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "learner.lr_find()\n",
        "learner.lr_plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "simulating training for different learning rates... this may take a few moments...\n",
            "Train on 859 samples\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ktrain/core.py:442: UserWarning: max_epochs is being set to 5 since steps per epoch is small. If you wish to estimate LR using more epochs, set max_epochs manually.\n",
            "  'If you wish to estimate LR using more epochs, set max_epochs manually.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "859/859 [==============================] - 1s 595us/sample - loss: 2.0523 - accuracy: 0.1490\n",
            "Epoch 2/5\n",
            "859/859 [==============================] - 0s 234us/sample - loss: 2.0216 - accuracy: 0.1607\n",
            "Epoch 3/5\n",
            "859/859 [==============================] - 0s 236us/sample - loss: 1.4816 - accuracy: 0.5169\n",
            "Epoch 4/5\n",
            "859/859 [==============================] - 0s 250us/sample - loss: 0.2863 - accuracy: 0.9336\n",
            "Epoch 5/5\n",
            "512/859 [================>.............] - ETA: 0s - loss: 5.5484 - accuracy: 0.8633\n",
            "\n",
            "done.\n",
            "Please invoke the Learner.lr_plot() method to visually inspect the loss plot to help identify the maximal learning rate associated with falling loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd5hU5d3/8fd3+y5sAXZpCyy9SWdt\nYMEuNmyJMYolGkUTS/TJY4wx5vnFRBNjjIZYsMYYxd5RFJVAVNClgxSp0lnKFraX+/fHjLji7rKL\ne/ZM+byuay5mzjlz5jPjuN+5z33OfZtzDhERiV4xfgcQERF/qRCIiEQ5FQIRkSinQiAiEuVUCERE\nopwKgYhIlIvzO0BzZWZmup49e/odQ0QkrMybN2+ncy6rvnVhVwh69uxJXl6e3zFERMKKmW1oaJ0O\nDYmIRDkVAhGRKKdCICIS5VQIRESinAqBiEiUUyEQEYlyKgSNqK6pZcW2IhZtLEDDdYtIpAq76wi8\nVlFdw+sLt/Bi3kaWbC6kvKoWgNE57bjppP6M6dMBM/M5pYhIy4nKQlBVU0tpRQ3pKfH7lm3cXcob\ni7bw9Kfr2V5UwYBOqfz4sByGdkujuLyaBz9aw0WPzWVg51TGDejIMf0zGZqdTmpSYB/OObYWlrNr\nbyV9OrYhJSEO5xxLNhcy44vtbC0sp7KmlupaR1bbRLq3TyGnfQrDuqXTMS3pOxk/WrmDBz9ajXPQ\nOT2J7Ixk+nVKZWDnVADeXbqNd5dtIy7GOHdUNmePzKZj6nf3s7/i8iq+2FLEks2FLNlcyMptxSTG\nx5KRHE96cjwpCbEkxcfSOT2JsX0yOaRrGjExKnwikczC7ZBHbm6uO5gri7cUlPHu0m38d/VO5q7d\nRUllDdkZyQzJTmNrYTmLNxUCMKZPByYd24ej+2V+65d/eVUNL+Zt5M3FW5m/YQ/VtYHPrV1KPJ3S\nkti8p4ziimoAYgx6ZbahrLKGLYXlxMYYHVMTSYiLITbG2FFUwd7gtgBd05MY2i2d/p1S6Z3VhtcX\nbmHmynxyOqTQNT2ZrYVlbCksp7K6dt9zYgyO7NOBssoa5n9VsO81UpPiaJMYR3WNo6K6hupaR1Jc\nLMkJsewuqWTdzpJ9++iclsSgLqnUOCgsraSwrIqyqhrKKmsoKq/e9/66pCdTXVtLTa2jXUoCHdMS\n6ZiaRPs2CbRvk0C3dskc1qs9KQnf/K6oqK5hbX4Jq7YXs2FXKalJcXRom0i7lG+KTdvEONKT40lN\niidWxUbEU2Y2zzmXW++6aCkE7yzZyjX/nk+vzDYc1TeTrhnJfLG1iKWbC0lLimP80C6MH9KZnA5t\nDriv4vIq5q7dzer8vXy1u5TtheVktwv8Yu/QJoGV24pZtqWIGIOTBnfixEGdaNcmYd/znXPsKa1i\nbf5eFm0qZOHGApZtKWTDrlJqah2piXHccGI/LjmyJwlxgW6cmlrH+l0lrNhaTGllNccN7Ehm20QA\n1uTv5Y2FW9hSUEZReRUlFTXExxpJ8bHExBgVVTWUV9XSJjGWIV3TOSQ7jSHZ6Y22IHYUl/PJ6l38\nd/VOCkqriIsxzGBPaSU7iivIL6rYV/gAEmJjOKxXe9omxrFqR/G+99IUZpDVNpGuGclkZyTTNjGO\n5IRY2rdJ4LgBHRmSnabDcSLfkwoBUFpZze6SSrq1S/EgVcuoqK5h3c4SuqQlf+uwVaiqrK6loLSS\nlduLmbUqn9lf7qSyupZ+ndrSr2Mq/TunMqBTKjkdUiitrGHX3gr2lFZRXlVDaWUNJRXVFJZVUVBa\nybaicjYXlLG1sJzSihpKK6sprqjGOcjOSOb4gR3J7dmO3J7t6ZqepMIg0kwqBBKW9pRUMmP5dt5d\nuo1P1+6itLIGCLQgvj7c1SYxltTEeNomxREfa8TGxJDVNpHjBwb6cb7uwxGJdioEEvYCp/IWM/+r\nPeQXV3yrVVFcXs3eimqqax3VtY4Nu0ooKK0iPtY4ul8W54/uxgmDOpIYF+v32xDxTWOFICrPGpLw\nExcbw5DsdIZkpx9w2+qaWuZ/VcCM5dt5Y+EWrl0xn/TkeCYekcNPj+4dFofdRFqTZy0CM+sOPA10\nAhwwxTl3/37bGHA/cBpQClzmnJvf2H7VIpDmqKl1fLx6J8/O/Yp3l20jNSmOK4/qzcQjc2hfpwNf\nJNL5cmjIzLoAXZxz880sFZgHnO2c+6LONqcB1xEoBIcD9zvnDm9svyoEcrC+2FLEfTNW8f4X20mI\ni+HMYV25dEwOw7pl+B1NxHO+HBpyzm0FtgbvF5vZciAb+KLOZhOAp12gGs0xswwz6xJ8rkiLGtw1\njUcvyWXltmL+NWc9r8zfzMvzNzGqRwaXjunJ+CFd9p2uKxJNWuVbb2Y9gZHA3P1WZQMb6zzeFFwm\n4pkBnVO58+yhzPn1Cdxx5mB2l1Ryw9SFHPeXmbw8b1OTr38QiRSeFwIzawu8DNzonCs6yH1cZWZ5\nZpaXn5/fsgElaqUlxXP52F58ePM4Hr80l4yUeG5+cRGnPzCb1xZspryqxu+IIq3C09NHzSweeAuY\n7pz7az3rHwFmOueeCz5eCYxr7NCQ+gjEK7W1jreXbOWv769i3c4S0pPjOWdkNlcc1Yvu7UP3QkSR\npvCrs9iAfwK7nXM3NrDN6cDP+aaz+AHn3GGN7VeFQLxWW+v4dO0upn6+kelLt1HjHOeOzObnx/dt\n0hAkIqHIr+sIxgITgSVmtjC47NdADwDn3MPANAJFYDWB00cv9zCPSJPExBhj+2Yytm8m2wrLefg/\na3jus694dcFmfnJUL647vq+uWJaIoiuLRZpgR3E5f31vFc/nbSSzbSK3nTaICSO6aswjCRuNtQh0\nrpxIE3RMTeLu84bx6rVj6ZqexI3PL+Tix+eyNn+v39FEvjcVApFmGNE9g1euHcvvzx7C4k2FnPq3\n2fzujWWsUUGQMKZDQyIHaUdxOX96ZyVvLNpMVY1jbN8O3HLqQF2pLCFJo4+KeCi/uIIX8jby1Cfr\n2bW3giuP7s0vTuxPcoJGO5XQoT4CEQ9lpSbys+P68sHNx3LBoT2YMmst4++fxeodxX5HE2kSFQKR\nFpKWFM9d5w7luZ8ewd6KGs5/+FPmbdjjdyyRA1IhEGlhR/bpwCvXjCEjOZ6LHpvDe8u2+R1JpFEq\nBCIe6NEhhZevGcOATqlc9a953DVtOZXVtX7HEqmXCoGIRzq0TeT5q4/kosN78Mistfzg4U90mqmE\nJBUCEQ8lxcfyh3OG8tBFo1i3s4ST75vFra8sZktBmd/RRPZRIRBpBeOHdmHGzccy8YgcXp63mXH3\nzOT+GV9SVaPDReI/FQKRVtIxNYnfnXUIH/7PsZwypDP3zVjFhMkfs2xLod/RJMqpEIi0sm7tUvj7\nhSOZMnE0+XsrmDD5Yx6bvZZwu7hTIocKgYhPTj6kM+//4hhOGNSRO99eztX/mkdhaZXfsSQKqRCI\n+CgjJYGHLx7N7WcM5sMVOzhz8n91ZpG0OhUCEZ+ZGVcc1Yvnrz6S0spqzn3wEz5bt9vvWBJFVAhE\nQsTonHa8eu1YOrRN4OLH5vLmoi1+R5IooUIgEkK6t0/hlWvGMKJHBtdPXcDUz77yO5JEARUCkRCT\nkZLA0z85jGP7Z/GrV5bw2Oy1fkeSCKdCIBKCkuJjmTIxl/FDOnPn28u5f8aXOr1UPKNCIBKiEuJi\n+PuFIzlvVDfum7GKP05brmIgnojzO4CINCwuNoZ7zh9G28RYHp29jr0V1dx59lBiY8zvaBJBVAhE\nQlxMjPG7sw6hbVIc//hoDWlJ8dx62iC/Y0kEUSEQCQNmxv+cPICismoembWW3lltuODQHn7Hkgih\nPgKRMGFm3HHmYI7ul8ltry5lztpdfkeSCKFCIBJG4mJjmPzjUeR0SGHSM/NYvaPY70gSATwrBGb2\nhJntMLOlDaxPN7M3zWyRmS0zs8u9yiISSdKT43nyssOIi4lh4uOfaZIb+d68bBE8BZzayPqfAV84\n54YD44B7zSzBwzwiEaNHhxT++ZND2VtezcTH57KnpNLvSBLGPCsEzrlZQGMjZzkg1cwMaBvcttqr\nPCKR5pCu6Tx6aS4b95Rx1b/yqKnVNQZycPzsI5gMDAK2AEuAG5xz9c7bZ2ZXmVmemeXl5+e3ZkaR\nkHZE7w7cfe5QPl+/h4f/s8bvOBKm/CwEpwALga7ACGCymaXVt6FzbopzLtc5l5uVldWaGUVC3jkj\nszljWBfue38VSzZp2ktpPj8LweXAKy5gNbAOGOhjHpGwZGb84eyhZLZN5MbnF1BWWeN3JAkzfhaC\nr4ATAMysEzAA0DCLIgchPSWee384nDX5Jdz1znK/40iY8fL00eeAT4EBZrbJzK4ws0lmNim4ye+B\nMWa2BPgAuMU5t9OrPCKRbmzfTK44qhdPf7qBj1bs8DuOhBELt9EMc3NzXV5ent8xREJSeVUNZ//j\nY3bureCdG44hKzXR70gSIsxsnnMut751urJYJIIkxcfywIUjKSqv5n9fWqRhq6VJVAhEIkz/Tqn8\nevxAPlqZzyOz1O0mB6ZCIBKBLh3Tk9OHdeFP767gwxXb/Y4jIU6FQCQCmRl/OX84h3RN4/rnFrJq\nuwank4apEIhEqOSEWB69JJfkhFiu/GcehWVVfkeSEKVCIBLBuqQn8/DFo9lcUMZtry5R57HUS4VA\nJMKNzmnHTSf1563FW3l5/ma/40gIUiEQiQKTju3D4b3a89vXl7JuZ4nfcSTEqBCIRIHYGOO+C0YQ\nHxvDL55fSK2GrJY6VAhEokTXjGR+e8ZgFm4s4LWFOkQk31AhEIki54zMZnj3DP707gpKKzUPlASo\nEIhEkZgY47dnDGZ7UQUPz9RENhKgQiASZUbntOOs4V15ZNZaNmvie0GFQCQq3TJ+IGbwp3dW+B1F\nQoAKgUgUys5I5sqjevPGoi0s3lTgdxzxmQqBSJS6+tjedGiTwB/eXq4rjqOcCoFIlEpNiufGE/sx\nd91uPliuGc2imQqBSBT70WE96J3ZhrveWU51Ta3fccQnKgQiUSw+NoZbxg9kTX4Jz+dt9DuO+ESF\nQCTKnTy4E6Nz2nH/jC8pq6zxO474QIVAJMqZGbecOpAdxRU89cl6v+OID1QIRITDerXnuAFZPDRz\nNYWlmsAm2qgQiAgAvzxlIEXl1TwyS0NPRBsVAhEBYHDXNCaM6MoTH69jR1G533GkFakQiMg+N53U\nn6oax0P/UasgmqgQiMg+OR3acN6obP499yu2FapVEC08KwRm9oSZ7TCzpY1sM87MFprZMjP7j1dZ\nRKTprju+H7W1jgdnrvY7irQSL1sETwGnNrTSzDKAB4GznHOHAD/wMIuINFH39in8ILc7Uz/byBYN\nUx0VPCsEzrlZwO5GNvkx8Ipz7qvg9hrsRCRE/Pz4vjgckz9SqyAa+NlH0B9oZ2YzzWyemV3S0IZm\ndpWZ5ZlZXn5+fitGFIlO2RnJXHBod17M28jWQrUKIp2fhSAOGA2cDpwC3G5m/evb0Dk3xTmX65zL\nzcrKas2MIlHr6mP6UOvgsdnr/I4iHvOzEGwCpjvnSpxzO4FZwHAf84hIHd3bpzBheFeenfsVu0sq\n/Y4jHvKzELwOHGVmcWaWAhwOLPcxj4js55pxfSirqtEYRBHOy9NHnwM+BQaY2SYzu8LMJpnZJADn\n3HLgXWAx8BnwmHOuwVNNRaT19euUysmDO/HPT9azt6La7zjikTivduycu7AJ29wD3ONVBhH5/q49\nri/vffExz87dwFXH9PE7jnhAVxaLSKNGdM/gqL6ZPPKftZSoVRCRVAhE5IBuOrk/u0oq1VcQoVQI\nROSARvVox4mDOvLwf9ZovoIIpEIgIk1y88kDKC6vZspsjUwaaVQIRKRJBnVJ48zhXXniv+vJL67w\nO460IBUCEWmyX5zYj8qaWo1MGmGaVAjM7AYzS7OAx81svpmd7HU4EQktvbPa7puvQGMQRY6mtgh+\n4pwrAk4G2gETgbs9SyUiIeu64/vhnGPyh2oVRIqmFgIL/nsa8C/n3LI6y0QkinRvn8IFh3bnhbyN\nbNxd6nccaQFNLQTzzOw9AoVgupmlArXexRKRUPbz4/phZjzwwZd+R5EW0NRCcAXwK+BQ51wpEA9c\n7lkqEQlpndOTuPjwHF5ZsJnVO4r9jiPfU1MLwZHASudcgZldDPwGKPQuloiEumuP60NKQix3vLEM\n55zfceR7aGoheAgoNbPhwM3AGuBpz1KJSMjLbJvIL08ZwMerd/HW4q1+x5HvoamFoNoFSv4EYLJz\n7h9AqnexRCQcXHR4DkOy07jz7S80THUYa2ohKDazWwmcNvq2mcUQ6CcQkSgWG2P8fsIQthdVcP+M\nVX7HkYPU1EJwAVBB4HqCbUA3NI+AiAAje7TjR4d258mP17NuZ4nfceQgNKkQBP/4/xtIN7MzgHLn\nnPoIRAQIDEiXEBfDX6av9DuKHISmDjHxQwLTSf4A+CEw18zO9zKYiISPrNREfnp0b95espWFGwv8\njiPN1NRDQ7cRuIbgUufcJcBhwO3exRKRcPPTY3qT2TaBu99ZrtNJw0xTC0GMc25Hnce7mvFcEYkC\nbRPjuP6EfsxZu5uZK/P9jiPN0NQ/5u+a2XQzu8zMLgPeBqZ5F0tEwtGPDu1Bzw4p/GHaciqrNQpN\nuGhqZ/EvgSnAsOBtinPuFi+DiUj4SYiL4fYzBrN6x16e+Hid33GkieKauqFz7mXgZQ+ziEgEOGFQ\nJ04a3In7Z3zJmcO7kp2R7HckOYBGWwRmVmxmRfXcis2sqLVCikh4uePMwTgc/+/NZX5HkSZotBA4\n51Kdc2n13FKdc2mtFVJEwku3dilcf0I/pi/bzkcrdhz4CeIrnfkjIp648qje9OyQwp/eXUFtrU4n\nDWWeFQIze8LMdpjZ0gNsd6iZVesCNZHIkhAXw40n9mfFtmKmLdXopKHMyxbBU8CpjW1gZrHAn4D3\nPMwhIj45c3hX+nVsy99mfEmNWgUhy7NC4JybBew+wGbXETgTSQcRRSJQbIxx44n9Wb1jL28u2uJ3\nHGmAb30EZpYNnENg0psDbXuVmeWZWV5+vq5YFAkn44d0ZmDnVO7/4Euqa3SRWSjys7P4b8AtzrkD\nfjOcc1Occ7nOudysrKxWiCYiLSUmxrjppP6s21nCi/M2+R1H6uFnIcgFpprZeuB84EEzO9vHPCLi\nkZMGd2J0Tjv++v4qSjSTWcjxrRA453o553o653oCLwHXOude8yuPiHjHzLjt9EHkF1cwZdZav+PI\nfrw8ffQ54FNggJltMrMrzGySmU3y6jVFJHSN6tGO04d2YcqstewoKvc7jtTR5LGGmss5d2Eztr3M\nqxwiEjr+99QBvPfFNv76/iruPm+Y33EkSFcWi0iryenQhkuO7MkLeRv5cnux33EkSIVARFrVz47r\nS0pCHH99f5XfUSRIhUBEWlX7NglceXQv3lm6jcWbNL9xKFAhEJFWd8VRvWiXEs9f3lOrIBSoEIhI\nq0tNiufacX2ZtSqfOWt3+R0n6qkQiIgvJh6ZQ+e0JP787gqc04B0flIhEBFfJMXHctNJ/Zn/VQFv\nLtYw1X5SIRAR35w3uhuHdE3j7mnLKa+q8TtO1FIhEBHfxMYYt58xmC2F5Rp6wkcqBCLiqyN6d2D8\nkM48NHMN2wo19IQfVAhExHe3jh9ETa3jz9NX+B0lKqkQiIjvenRI4fKxPXl1wWaWbi70O07UUSEQ\nkZBw7XF9SU+O54/Tlut00lamQiAiISE9OZ4bTujHJ2t2MXOlpqRtTSoEIhIyLjo8h54dUvjjtOWa\n37gVqRCISMhIiIvhV+MH8uWOvTz96Qa/40QNFQIRCSmnHNKZ4wZk8efpK1i/s8TvOFFBhUBEQoqZ\ncde5w4iPjeGXLy2itlYdx15TIRCRkNM5PYk7zjyEz9fv4clP1vsdJ+KpEIhISDpvVDYnDOzIPdNX\nsHF3qd9xIpoKgYiEJDPjznOGAHD3u7ri2EsqBCISsrqkJzPp2D68vXgreet3+x0nYqkQiEhIu+qY\n3nROS+L/vfWFOo49okIgIiEtJSGOW8YPYPGmQl5dsNnvOBFJhUBEQt6E4dkM757Bn6evoLSy2u84\nEUeFQERCXkyM8dszBrG9qIKHZ67xO44vXl+4mWVbvBmZ1bNCYGZPmNkOM1vawPqLzGyxmS0xs0/M\nbLhXWUQk/I3Oac+Zw7vyyKy1bC4o8ztOq3LOcfMLi3jLo7mdvWwRPAWc2sj6dcCxzrmhwO+BKR5m\nEZEI8KvxAwG4+53oOp20tLKG6lpHRnK8J/v3rBA452YBDZ7v5Zz7xDm3J/hwDtDNqywiEhmyM5K5\n+pjevLloC/M2RM/ppAVlVUBgqG4vhEofwRXAOw2tNLOrzCzPzPLy8zVOuUg0u/rYPnROS+LmFxax\np6TS7zitorA0UAgyUiK0EJjZcQQKwS0NbeOcm+Kcy3XO5WZlZbVeOBEJOW0S4/jHRSPZUljO1c/M\no7I68uctKCgLFLy0SGwRmNkw4DFggnNul59ZRCR8jM5pzz3nD+Ozdbu59ZUlET+1ZVHw0FBGcoIn\n+4/zZK9NYGY9gFeAic65VX7lEJHwNGFENut3lnLfjFX06diGa8f19TuSZwqCh4bSPTo05FkhMLPn\ngHFAppltAu4A4gGccw8DvwU6AA+aGUC1cy7XqzwiEnmuP6Eva/L3cs/0lfTvmMqJgzv5HckThfta\nBGFWCJxzFx5g/ZXAlV69vohEPjPjz+cPY/2uEm6YuoBXrh3LgM6pfsdqcYVlVcTFGCkJsZ7s3/fO\nYhGR7yMpPpYpE3NJSYzjyqc/j8gziQrKqshIiSd49KTFqRCISNjrnJ7ElImj2V5YwU0vLIy4UUoL\ny6o8O2MIVAhEJEKM7NGO288YxEcr83noP5E1HlFhaZVn/QOgQiAiEeTiI3I4a3hX7n1vJZ+s3ul3\nnBZTWFbl2VXFoEIgIhHEzLjr3KH0ymzD9VMXsjtC+gsKyirJSPHmGgJQIRCRCNMmMY7JPx5FYVkl\nv3ktMi42KyxVi0BEpFkGdUnjxhP7M23JNt70aOjm1lJT6ygqr1YhEBFprquP6c2I7hn89vWl7Cgq\n9zvOQSsu93bkUVAhEJEIFRcbw70/HE5ZZQ3XPbcgbKe4LPB45FFQIRCRCNYnqy1/Pn8Yn6/fzWVP\nfk5JRfgVg0KP5yIAFQIRiXATRmRz3wUjmLdhD5c+8dm+Qy3h4utJadQiEBH5HiaMyOaBH41k4cYC\nrn9uQVhdeawWgYhICzl9WBfuOHMwH63M54EPv/Q7TpMVlgauhUj3aC4CUCEQkShy8RE5nDsqm/s/\n+JKPVuzwO06TqEUgItKCzIw/njOUQZ3TuGHqAv75yfp98wGHqoLSKlISYkmI8+7PtQqBiESVpPhY\nHpk4mpwObbjjjWUc+scZ3PrKEmpCtN/A63GGwMepKkVE/NK9fQpvXncUSzcX8sycDTz32Vf0ykzh\nqmP6+B3tOwpaoRCoRSAiUWtIdjp3nTuUkwd34i/vrWJN/l6/I31Ha7QIVAhEJKqZGXeeM4Tk+Fj+\n96XFIXeIyOsB50CFQESEjqlJ3HHmYOZt2MNDM1eH1IilhcFpKr2kQiAiApwzMptTD+nMX95bxYWP\nzmHFtiK/IwGBuQjUIhARaQVmxj8uGsUfzhnCim3FnHb/bK799zymLdlKWWWNL5nKq2oor6r1dFIa\n0FlDIiL7xMYYFx2ew+lDuzD5w9W8tnAz05Zso01CLA9cOJITBnVq1TxFwYvJvJy4HtQiEBH5joyU\nBH5zxmDm/vpEnr3ycHplteHnzy5g4caCVs3x9VXFXk5cDyoEIiINio0xxvTN5MnLDiMrNZErnvqc\nDbtKWu31C1pheAnwsBCY2RNmtsPMljaw3szsATNbbWaLzWyUV1lERL6PrNREnrr8UGqd47InP6eo\nlYayLmyFSWnA2xbBU8CpjawfD/QL3q4CHvIwi4jI99I7qy2PXpLLV7tL+e1r9f6+bXFh3yJwzs0C\ndjeyyQTgaRcwB8gwsy5e5RER+b5ye7bn+uP78drCLby+cLPnr/dNH4G3Zw352UeQDWys83hTcJmI\nSMj62XF9GJ3Tjt+8upRNe0o9fa3C0krMIDXJ2xM8w6Kz2MyuMrM8M8vLz8/3O46IRLG42Bju++EI\nHHDW5I854++z+dGUT3lr8ZYWf63CsirSkuKJibEW33ddfhaCzUD3Oo+7BZd9h3NuinMu1zmXm5WV\n1SrhREQa0qNDClMmjubofpl0TE1ie1EFN05dyLwNjR0Nb77WGHkU/L2g7A3g52Y2FTgcKHTObfUx\nj4hIk43pm8mYvplA4Jf7WZP/yzXPzOet64+iY2rSQe93W2E5j/93LVsKyvl07S6yM5JbKnKDvDx9\n9DngU2CAmW0ysyvMbJKZTQpuMg1YC6wGHgWu9SqLiIiX0pPjefji0RSXV/Pzfy+gqqb2oPf129eX\n8uTH61m+tYiBnVO5bEzPlgvaAM9aBM65Cw+w3gE/8+r1RURa06Auadx93lBumLqQe6av5NenDWr2\nPjYXlDFj+XauPrYPt5w60IOU9QuLzmIRkXAwYUQ2E4/IYcqstXy0ckezn//s3A0AXHR4j5aO1igV\nAhGRFnTb6YMY2DmVm19YxPai8ka3LS6v2jf3QUV1DVM/28jxAzvRrV1Ka0TdR4VARKQFJcXHMvnH\nIymrrOHGqQspr6p/COv1O0s44o8fcMkTn1FYWsU7S7axq6SSS8fktHJiFQIRkRbXt2Mqvz97CJ+u\n3cX4+2czd+2ub613znH760txwJy1uzjrH//loZlr6J3ZhrF9Mls9rwqBiIgHzh/djWeuOJzq2lou\nmDKH219bum+Cm7eXbGX2lzv531MGMPWqIyipqGHl9mIuPiLH84vH6mOhNDdnU+Tm5rq8vDy/Y4iI\nNElpZTX3TF/Jkx+vp3+nttx17lCueWY+WamJvP6zscTFxrCtsJxXF2zmsjE9SU6I9SSHmc1zzuXW\nu06FQETEe7NW5XPTC4vYubcCM3j12rGM6J7Raq/fWCHQVJUiIq3gmP5ZvHvj0fzujWX065jaqkXg\nQFQIRERaSWbbRCb/OPTm4EB9K9gAAAkYSURBVFJnsYhIlFMhEBGJcioEIiJRToVARCTKqRCIiEQ5\nFQIRkSinQiAiEuVUCEREolzYDTFhZvnABiAdKGxk04bW77+8scf13f/630xgZzPjN5arKduEevam\n5m4sa3156y7zO/vBfuYQvtn9zl3fsgN91qGa3c/vS45zLqveZzjnwvIGTDmY9fsvb+xxfffr/Jvn\nRe5wzt7U3I1lrS9vKGU/2M88nLP7nbup2UPx+9KUz7m1vy/13cL50NCbB7l+/+WNPa7v/oFe90Ca\n8vxwzd7U3Psva+h9NLZNc4Xr96Up+wjV7F59X/Z/HMrfl/2XhcL35TvC7tBQqDCzPNfASH6hTtn9\nEa7ZwzU3KHtThXOLwG9T/A7wPSi7P8I1e7jmBmVvErUIRESinFoEIiJRToVARCTKqRCIiEQ5FQIP\nmFmMmf3BzP5uZpf6nac5zGycmc02s4fNbJzfeZrDzNqYWZ6ZneF3luYws0HBz/slM7vG7zzNYWZn\nm9mjZva8mZ3sd57mMLPeZva4mb3kd5YDCX63/xn8rC9q6f2rEOzHzJ4wsx1mtnS/5aea2UozW21m\nvzrAbiYA3YAqYJNXWffXQtkdsBdIopWyt1BugFuAF7xJWb+WyO6cW+6cmwT8EBjrZd66Wij7a865\nnwKTgAu8zFtXC2Vf65y7wtukDWvmezgXeCn4WZ/V4mEO5sq1SL4BxwCjgKV1lsUCa4DeQAKwCBgM\nDAXe2u/WEfgVcHXwuS+FWfaY4PM6Af8Oo9wnAT8CLgPOCKfPPPics4B3gB+HW/bg8+4FRoVp9lb7\nf/R7vIdbgRHBbZ5t6SyavH4/zrlZZtZzv8WHAaudc2sBzGwqMME5dxfwncMQZrYJqAw+rPEu7be1\nRPY69gCJXuTcXwt95uOANgT+pykzs2nOuVovc0PLfebOuTeAN8zsbeBZ7xJ/6zVb4nM34G7gHefc\nfG8Tf6OFv+u+aM57INA67wYsxIMjOSoETZMNbKzzeBNweCPbvwL83cyOBmZ5GawJmpXdzM4FTgEy\ngMneRmtUs3I7524DMLPLgJ2tUQQa0dzPfByBpn8iMM3TZAfW3O/6dcCJQLqZ9XXOPexluANo7ufe\nAfgDMNLMbg0WDL819B4eACab2em0zDAU36JC4AHnXCng27HH78M59wqBQhaWnHNP+Z2huZxzM4GZ\nPsc4KM65Bwj8kQo7zrldBPo2Qp5zrgS43Kv9q7O4aTYD3es87hZcFg7CNXu45gZl90s4Z/+aL+9B\nhaBpPgf6mVkvM0sg0Cn5hs+Zmipcs4drblB2v4Rz9q/58x786C0P5RvwHLCVb079vCK4/DRgFYEe\n/dv8zhlJ2cM1t7Ire6S8Bw06JyIS5XRoSEQkyqkQiIhEORUCEZEop0IgIhLlVAhERKKcCoGISJRT\nIRDPmdneVniNs5o4VHVLvuY4MxtzEM8baWaPB+9fZmZ+jum0j5n13H9I5Hq2yTKzd1srk7QOFQIJ\nG2YW29A659wbzrm7PXjNxsbjGgc0uxAAvyZ8x+fJB7aaWavNmyDeUyGQVmVmvzSzz81ssZn9X53l\nr5nZPDNbZmZX1Vm+18zuNbNFwJFmtt7M/s/M5pvZEjMbGNxu3y9rM3vKzB4ws0/MbK2ZnR9cHmNm\nD5rZCjN738ymfb1uv4wzzexvZpYH3GBmZ5rZXDNbYGYzzKxTcPjgScAvzGyhmR0d/LX8cvD9fV7f\nH0szSwWGOecW1bOup5l9GPxsPjCzHsHlfcxsTvD93llfC8sCM1i9bWaLzGypmV0QXH5o8HNYZGaf\nmVlq8HVmBz/D+fW1asws1szuqfPf6uo6q18DWnyWLPGR35dZ6xb5N2Bv8N+TgSmAEfgR8hZwTHBd\n++C/ycBSoEPwsQN+WGdf64HrgvevBR4L3r8MmBy8/xTwYvA1BhMY3x3gfALDPMcAnQnMuXB+PXln\nAg/WedwO9l2FfyVwb/D+74D/qbPds8BRwfs9gOX17Ps44OU6j+vmfhO4NHj/J8BrwftvARcG70/6\n+vPcb7/nAY/WeZxOYGKTtcChwWVpBEYcTgGSgsv6AXnB+z0JTpICXAX8Jng/EcgDegUfZwNL/P5e\n6dZyNw1DLa3p5OBtQfBxWwJ/iGYB15vZOcHl3YPLdxGY2Ofl/fbz9TDZ8wiM41+f11xgToIvzKxT\ncNlRwIvB5dvM7KNGsj5f53434Hkz60Lgj+u6Bp5zIjDYzL5+nGZmbZ1zdX/BdwHyG3j+kXXez7+A\nP9dZfnbw/rPAX+p57hLgXjP7E/CWc262mQ0FtjrnPgdwzhVBoPVAYGz7EQQ+3/717O9kYFidFlM6\ngf8m64AdQNcG3oOEIRUCaU0G3OWce+RbCwMTs5wIHOmcKzWzmQTmTAYod87tP8tbRfDfGhr+DlfU\nuW8NbNOYkjr3/w781Tn3RjDr7xp4TgxwhHOuvJH9lvHNe2sxzrlVZjaKwIBld5rZB8CrDWz+C2A7\nMJxA5vryGoGW1/R61iUReB8SIdRHIK1pOvATM2sLYGbZZtaRwK/NPcEiMBA4wqPX/xg4L9hX0IlA\nZ29TpPPNmPCX1lleDKTWefwegRm7AAj+4t7fcqBvA6/zCYFhhyFwDH528P4cAod+qLP+W8ysK1Dq\nnHsGuIfAXLgrgS5mdmhwm9Rg53c6gZZCLTCRwDy5+5sOXGNm8cHn9g+2JCDQgmj07CIJLyoE0mqc\nc+8ROLTxqZktAV4i8If0XSDOzJYTmP92jkcRXiYw3O8XwDPAfKCwCc/7HfCimc0DdtZZ/iZwzted\nxcD1QG6wc/UL6pn9yjm3gsC0jqn7ryNQRC43s8UE/kDfEFx+I3BTcHnfBjIPBT4zs4XAHcCdzrlK\n4AIC06YuAt4n8Gv+QeDS4LKBfLv187XHCHxO84OnlD7CN62v44C363mOhCkNQy1R5etj9haYr/Yz\nYKxzblsrZ/gFUOyce6yJ26cAZc45Z2Y/ItBxPMHTkI3nmUVgUvg9fmWQlqU+Aok2b5lZBoFO39+3\ndhEIegj4QTO2H02gc9eAAgJnFPnCzLII9JeoCEQQtQhERKKc+ghERKKcCoGISJRTIRARiXIqBCIi\nUU6FQEQkyqkQiIhEuf8PxbTQGnmuuPYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "venMMOdM3Drl",
        "colab_type": "code",
        "outputId": "43b690a7-67dc-4461-cd76-55234af79a93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learner.fit_onecycle(0.0001, 200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 0.0001...\n",
            "Train on 859 samples, validate on 115 samples\n",
            "Epoch 1/200\n",
            "859/859 [==============================] - 0s 369us/sample - loss: 2.0439 - accuracy: 0.1548 - val_loss: 2.0783 - val_accuracy: 0.1652\n",
            "Epoch 2/200\n",
            "859/859 [==============================] - 0s 283us/sample - loss: 2.0077 - accuracy: 0.1688 - val_loss: 2.0477 - val_accuracy: 0.1565\n",
            "Epoch 3/200\n",
            "859/859 [==============================] - 0s 300us/sample - loss: 1.9642 - accuracy: 0.1932 - val_loss: 2.0154 - val_accuracy: 0.1913\n",
            "Epoch 4/200\n",
            "859/859 [==============================] - 0s 280us/sample - loss: 1.9206 - accuracy: 0.2317 - val_loss: 1.9831 - val_accuracy: 0.2348\n",
            "Epoch 5/200\n",
            "859/859 [==============================] - 0s 270us/sample - loss: 1.8777 - accuracy: 0.2829 - val_loss: 1.9518 - val_accuracy: 0.2696\n",
            "Epoch 6/200\n",
            "859/859 [==============================] - 0s 255us/sample - loss: 1.8360 - accuracy: 0.3260 - val_loss: 1.9216 - val_accuracy: 0.3130\n",
            "Epoch 7/200\n",
            "859/859 [==============================] - 0s 280us/sample - loss: 1.7961 - accuracy: 0.3725 - val_loss: 1.8916 - val_accuracy: 0.3739\n",
            "Epoch 8/200\n",
            "859/859 [==============================] - 0s 302us/sample - loss: 1.7567 - accuracy: 0.4075 - val_loss: 1.8633 - val_accuracy: 0.3739\n",
            "Epoch 9/200\n",
            "859/859 [==============================] - 0s 268us/sample - loss: 1.7185 - accuracy: 0.4377 - val_loss: 1.8349 - val_accuracy: 0.3826\n",
            "Epoch 10/200\n",
            "859/859 [==============================] - 0s 270us/sample - loss: 1.6805 - accuracy: 0.4575 - val_loss: 1.8073 - val_accuracy: 0.4087\n",
            "Epoch 11/200\n",
            "859/859 [==============================] - 0s 300us/sample - loss: 1.6434 - accuracy: 0.4703 - val_loss: 1.7801 - val_accuracy: 0.4261\n",
            "Epoch 12/200\n",
            "859/859 [==============================] - 0s 266us/sample - loss: 1.6065 - accuracy: 0.4878 - val_loss: 1.7533 - val_accuracy: 0.4609\n",
            "Epoch 13/200\n",
            "859/859 [==============================] - 0s 265us/sample - loss: 1.5701 - accuracy: 0.5052 - val_loss: 1.7266 - val_accuracy: 0.4696\n",
            "Epoch 14/200\n",
            "859/859 [==============================] - 0s 279us/sample - loss: 1.5338 - accuracy: 0.5239 - val_loss: 1.7004 - val_accuracy: 0.4870\n",
            "Epoch 15/200\n",
            "859/859 [==============================] - 0s 261us/sample - loss: 1.4978 - accuracy: 0.5425 - val_loss: 1.6758 - val_accuracy: 0.4957\n",
            "Epoch 16/200\n",
            "859/859 [==============================] - 0s 285us/sample - loss: 1.4619 - accuracy: 0.5704 - val_loss: 1.6504 - val_accuracy: 0.5043\n",
            "Epoch 17/200\n",
            "859/859 [==============================] - 0s 268us/sample - loss: 1.4267 - accuracy: 0.5832 - val_loss: 1.6245 - val_accuracy: 0.5304\n",
            "Epoch 18/200\n",
            "859/859 [==============================] - 0s 265us/sample - loss: 1.3914 - accuracy: 0.6205 - val_loss: 1.6004 - val_accuracy: 0.5391\n",
            "Epoch 19/200\n",
            "859/859 [==============================] - 0s 304us/sample - loss: 1.3562 - accuracy: 0.6426 - val_loss: 1.5765 - val_accuracy: 0.5478\n",
            "Epoch 20/200\n",
            "859/859 [==============================] - 0s 305us/sample - loss: 1.3217 - accuracy: 0.6647 - val_loss: 1.5532 - val_accuracy: 0.5739\n",
            "Epoch 21/200\n",
            "859/859 [==============================] - 0s 283us/sample - loss: 1.2873 - accuracy: 0.6938 - val_loss: 1.5309 - val_accuracy: 0.5652\n",
            "Epoch 22/200\n",
            "859/859 [==============================] - 0s 259us/sample - loss: 1.2536 - accuracy: 0.7148 - val_loss: 1.5077 - val_accuracy: 0.5739\n",
            "Epoch 23/200\n",
            "859/859 [==============================] - 0s 263us/sample - loss: 1.2200 - accuracy: 0.7369 - val_loss: 1.4854 - val_accuracy: 0.5739\n",
            "Epoch 24/200\n",
            "859/859 [==============================] - 0s 292us/sample - loss: 1.1872 - accuracy: 0.7567 - val_loss: 1.4638 - val_accuracy: 0.5826\n",
            "Epoch 25/200\n",
            "859/859 [==============================] - 0s 266us/sample - loss: 1.1548 - accuracy: 0.7835 - val_loss: 1.4431 - val_accuracy: 0.5826\n",
            "Epoch 26/200\n",
            "859/859 [==============================] - 0s 271us/sample - loss: 1.1229 - accuracy: 0.7951 - val_loss: 1.4206 - val_accuracy: 0.6087\n",
            "Epoch 27/200\n",
            "859/859 [==============================] - 0s 274us/sample - loss: 1.0918 - accuracy: 0.8114 - val_loss: 1.4003 - val_accuracy: 0.6174\n",
            "Epoch 28/200\n",
            "859/859 [==============================] - 0s 275us/sample - loss: 1.0609 - accuracy: 0.8242 - val_loss: 1.3799 - val_accuracy: 0.6174\n",
            "Epoch 29/200\n",
            "859/859 [==============================] - 0s 266us/sample - loss: 1.0308 - accuracy: 0.8393 - val_loss: 1.3610 - val_accuracy: 0.6348\n",
            "Epoch 30/200\n",
            "859/859 [==============================] - 0s 275us/sample - loss: 1.0016 - accuracy: 0.8556 - val_loss: 1.3413 - val_accuracy: 0.6522\n",
            "Epoch 31/200\n",
            "859/859 [==============================] - 0s 273us/sample - loss: 0.9729 - accuracy: 0.8615 - val_loss: 1.3218 - val_accuracy: 0.6522\n",
            "Epoch 32/200\n",
            "859/859 [==============================] - 0s 280us/sample - loss: 0.9447 - accuracy: 0.8836 - val_loss: 1.3037 - val_accuracy: 0.6696\n",
            "Epoch 33/200\n",
            "859/859 [==============================] - 0s 306us/sample - loss: 0.9176 - accuracy: 0.8964 - val_loss: 1.2860 - val_accuracy: 0.6696\n",
            "Epoch 34/200\n",
            "859/859 [==============================] - 0s 270us/sample - loss: 0.8908 - accuracy: 0.9104 - val_loss: 1.2682 - val_accuracy: 0.6696\n",
            "Epoch 35/200\n",
            "859/859 [==============================] - 0s 277us/sample - loss: 0.8649 - accuracy: 0.9208 - val_loss: 1.2519 - val_accuracy: 0.6696\n",
            "Epoch 36/200\n",
            "859/859 [==============================] - 0s 263us/sample - loss: 0.8398 - accuracy: 0.9290 - val_loss: 1.2350 - val_accuracy: 0.6696\n",
            "Epoch 37/200\n",
            "859/859 [==============================] - 0s 281us/sample - loss: 0.8150 - accuracy: 0.9313 - val_loss: 1.2180 - val_accuracy: 0.6609\n",
            "Epoch 38/200\n",
            "859/859 [==============================] - 0s 285us/sample - loss: 0.7910 - accuracy: 0.9371 - val_loss: 1.2030 - val_accuracy: 0.6696\n",
            "Epoch 39/200\n",
            "859/859 [==============================] - 0s 289us/sample - loss: 0.7681 - accuracy: 0.9406 - val_loss: 1.1879 - val_accuracy: 0.6783\n",
            "Epoch 40/200\n",
            "859/859 [==============================] - 0s 278us/sample - loss: 0.7457 - accuracy: 0.9453 - val_loss: 1.1723 - val_accuracy: 0.6870\n",
            "Epoch 41/200\n",
            "859/859 [==============================] - 0s 282us/sample - loss: 0.7237 - accuracy: 0.9499 - val_loss: 1.1575 - val_accuracy: 0.6870\n",
            "Epoch 42/200\n",
            "859/859 [==============================] - 0s 268us/sample - loss: 0.7024 - accuracy: 0.9534 - val_loss: 1.1443 - val_accuracy: 0.6870\n",
            "Epoch 43/200\n",
            "859/859 [==============================] - 0s 265us/sample - loss: 0.6821 - accuracy: 0.9569 - val_loss: 1.1311 - val_accuracy: 0.6870\n",
            "Epoch 44/200\n",
            "859/859 [==============================] - 0s 278us/sample - loss: 0.6622 - accuracy: 0.9639 - val_loss: 1.1188 - val_accuracy: 0.6870\n",
            "Epoch 45/200\n",
            "859/859 [==============================] - 0s 289us/sample - loss: 0.6429 - accuracy: 0.9651 - val_loss: 1.1046 - val_accuracy: 0.6870\n",
            "Epoch 46/200\n",
            "859/859 [==============================] - 0s 292us/sample - loss: 0.6243 - accuracy: 0.9662 - val_loss: 1.0928 - val_accuracy: 0.6870\n",
            "Epoch 47/200\n",
            "859/859 [==============================] - 0s 276us/sample - loss: 0.6061 - accuracy: 0.9674 - val_loss: 1.0807 - val_accuracy: 0.6783\n",
            "Epoch 48/200\n",
            "859/859 [==============================] - 0s 268us/sample - loss: 0.5892 - accuracy: 0.9709 - val_loss: 1.0699 - val_accuracy: 0.6696\n",
            "Epoch 49/200\n",
            "859/859 [==============================] - 0s 263us/sample - loss: 0.5720 - accuracy: 0.9744 - val_loss: 1.0590 - val_accuracy: 0.6696\n",
            "Epoch 50/200\n",
            "859/859 [==============================] - 0s 298us/sample - loss: 0.5556 - accuracy: 0.9709 - val_loss: 1.0473 - val_accuracy: 0.6696\n",
            "Epoch 51/200\n",
            "859/859 [==============================] - 0s 337us/sample - loss: 0.5396 - accuracy: 0.9756 - val_loss: 1.0366 - val_accuracy: 0.6696\n",
            "Epoch 52/200\n",
            "859/859 [==============================] - 0s 277us/sample - loss: 0.5245 - accuracy: 0.9779 - val_loss: 1.0277 - val_accuracy: 0.6696\n",
            "Epoch 53/200\n",
            "859/859 [==============================] - 0s 263us/sample - loss: 0.5097 - accuracy: 0.9779 - val_loss: 1.0175 - val_accuracy: 0.6696\n",
            "Epoch 54/200\n",
            "859/859 [==============================] - 0s 296us/sample - loss: 0.4952 - accuracy: 0.9779 - val_loss: 1.0076 - val_accuracy: 0.6696\n",
            "Epoch 55/200\n",
            "859/859 [==============================] - 0s 266us/sample - loss: 0.4813 - accuracy: 0.9779 - val_loss: 0.9983 - val_accuracy: 0.6696\n",
            "Epoch 56/200\n",
            "859/859 [==============================] - 0s 258us/sample - loss: 0.4679 - accuracy: 0.9825 - val_loss: 0.9904 - val_accuracy: 0.6696\n",
            "Epoch 57/200\n",
            "859/859 [==============================] - 0s 266us/sample - loss: 0.4550 - accuracy: 0.9825 - val_loss: 0.9823 - val_accuracy: 0.6696\n",
            "Epoch 58/200\n",
            "859/859 [==============================] - 0s 294us/sample - loss: 0.4423 - accuracy: 0.9837 - val_loss: 0.9742 - val_accuracy: 0.6696\n",
            "Epoch 59/200\n",
            "859/859 [==============================] - 0s 268us/sample - loss: 0.4302 - accuracy: 0.9837 - val_loss: 0.9654 - val_accuracy: 0.6696\n",
            "Epoch 60/200\n",
            "859/859 [==============================] - 0s 273us/sample - loss: 0.4184 - accuracy: 0.9837 - val_loss: 0.9578 - val_accuracy: 0.6696\n",
            "Epoch 61/200\n",
            "859/859 [==============================] - 0s 289us/sample - loss: 0.4070 - accuracy: 0.9849 - val_loss: 0.9509 - val_accuracy: 0.6696\n",
            "Epoch 62/200\n",
            "859/859 [==============================] - 0s 293us/sample - loss: 0.3960 - accuracy: 0.9860 - val_loss: 0.9448 - val_accuracy: 0.6696\n",
            "Epoch 63/200\n",
            "859/859 [==============================] - 0s 294us/sample - loss: 0.3853 - accuracy: 0.9860 - val_loss: 0.9369 - val_accuracy: 0.6696\n",
            "Epoch 64/200\n",
            "859/859 [==============================] - 0s 291us/sample - loss: 0.3750 - accuracy: 0.9860 - val_loss: 0.9305 - val_accuracy: 0.6696\n",
            "Epoch 65/200\n",
            "859/859 [==============================] - 0s 275us/sample - loss: 0.3649 - accuracy: 0.9860 - val_loss: 0.9233 - val_accuracy: 0.6696\n",
            "Epoch 66/200\n",
            "859/859 [==============================] - 0s 285us/sample - loss: 0.3553 - accuracy: 0.9872 - val_loss: 0.9179 - val_accuracy: 0.6696\n",
            "Epoch 67/200\n",
            "859/859 [==============================] - 0s 270us/sample - loss: 0.3459 - accuracy: 0.9895 - val_loss: 0.9115 - val_accuracy: 0.6696\n",
            "Epoch 68/200\n",
            "859/859 [==============================] - 0s 265us/sample - loss: 0.3367 - accuracy: 0.9907 - val_loss: 0.9066 - val_accuracy: 0.6696\n",
            "Epoch 69/200\n",
            "859/859 [==============================] - 0s 281us/sample - loss: 0.3281 - accuracy: 0.9907 - val_loss: 0.8999 - val_accuracy: 0.6696\n",
            "Epoch 70/200\n",
            "859/859 [==============================] - 0s 262us/sample - loss: 0.3193 - accuracy: 0.9907 - val_loss: 0.8960 - val_accuracy: 0.6696\n",
            "Epoch 71/200\n",
            "859/859 [==============================] - 0s 294us/sample - loss: 0.3113 - accuracy: 0.9907 - val_loss: 0.8897 - val_accuracy: 0.6696\n",
            "Epoch 72/200\n",
            "859/859 [==============================] - 0s 279us/sample - loss: 0.3032 - accuracy: 0.9907 - val_loss: 0.8855 - val_accuracy: 0.6696\n",
            "Epoch 73/200\n",
            "859/859 [==============================] - 0s 269us/sample - loss: 0.2953 - accuracy: 0.9907 - val_loss: 0.8792 - val_accuracy: 0.6696\n",
            "Epoch 74/200\n",
            "859/859 [==============================] - 0s 279us/sample - loss: 0.2877 - accuracy: 0.9907 - val_loss: 0.8755 - val_accuracy: 0.6696\n",
            "Epoch 75/200\n",
            "859/859 [==============================] - 0s 306us/sample - loss: 0.2806 - accuracy: 0.9919 - val_loss: 0.8711 - val_accuracy: 0.6696\n",
            "Epoch 76/200\n",
            "859/859 [==============================] - 0s 295us/sample - loss: 0.2735 - accuracy: 0.9930 - val_loss: 0.8663 - val_accuracy: 0.6696\n",
            "Epoch 77/200\n",
            "859/859 [==============================] - 0s 268us/sample - loss: 0.2664 - accuracy: 0.9953 - val_loss: 0.8619 - val_accuracy: 0.6696\n",
            "Epoch 78/200\n",
            "859/859 [==============================] - 0s 265us/sample - loss: 0.2597 - accuracy: 0.9953 - val_loss: 0.8575 - val_accuracy: 0.6609\n",
            "Epoch 79/200\n",
            "859/859 [==============================] - 0s 286us/sample - loss: 0.2534 - accuracy: 0.9965 - val_loss: 0.8535 - val_accuracy: 0.6696\n",
            "Epoch 80/200\n",
            "859/859 [==============================] - 0s 291us/sample - loss: 0.2469 - accuracy: 0.9965 - val_loss: 0.8500 - val_accuracy: 0.6609\n",
            "Epoch 81/200\n",
            "859/859 [==============================] - 0s 262us/sample - loss: 0.2409 - accuracy: 0.9965 - val_loss: 0.8465 - val_accuracy: 0.6696\n",
            "Epoch 82/200\n",
            "859/859 [==============================] - 0s 259us/sample - loss: 0.2351 - accuracy: 0.9965 - val_loss: 0.8428 - val_accuracy: 0.6609\n",
            "Epoch 83/200\n",
            "859/859 [==============================] - 0s 284us/sample - loss: 0.2293 - accuracy: 0.9965 - val_loss: 0.8381 - val_accuracy: 0.6696\n",
            "Epoch 84/200\n",
            "859/859 [==============================] - 0s 281us/sample - loss: 0.2237 - accuracy: 0.9965 - val_loss: 0.8363 - val_accuracy: 0.6696\n",
            "Epoch 85/200\n",
            "859/859 [==============================] - 0s 279us/sample - loss: 0.2182 - accuracy: 0.9965 - val_loss: 0.8324 - val_accuracy: 0.6609\n",
            "Epoch 86/200\n",
            "859/859 [==============================] - 0s 300us/sample - loss: 0.2130 - accuracy: 0.9965 - val_loss: 0.8301 - val_accuracy: 0.6609\n",
            "Epoch 87/200\n",
            "859/859 [==============================] - 0s 309us/sample - loss: 0.2078 - accuracy: 0.9965 - val_loss: 0.8267 - val_accuracy: 0.6609\n",
            "Epoch 88/200\n",
            "859/859 [==============================] - 0s 309us/sample - loss: 0.2029 - accuracy: 0.9965 - val_loss: 0.8236 - val_accuracy: 0.6609\n",
            "Epoch 89/200\n",
            "859/859 [==============================] - 0s 293us/sample - loss: 0.1980 - accuracy: 0.9977 - val_loss: 0.8213 - val_accuracy: 0.6609\n",
            "Epoch 90/200\n",
            "859/859 [==============================] - 0s 276us/sample - loss: 0.1933 - accuracy: 0.9977 - val_loss: 0.8172 - val_accuracy: 0.6609\n",
            "Epoch 91/200\n",
            "859/859 [==============================] - 0s 291us/sample - loss: 0.1888 - accuracy: 0.9977 - val_loss: 0.8150 - val_accuracy: 0.6609\n",
            "Epoch 92/200\n",
            "859/859 [==============================] - 0s 279us/sample - loss: 0.1843 - accuracy: 0.9977 - val_loss: 0.8118 - val_accuracy: 0.6609\n",
            "Epoch 93/200\n",
            "859/859 [==============================] - 0s 311us/sample - loss: 0.1799 - accuracy: 0.9988 - val_loss: 0.8101 - val_accuracy: 0.6609\n",
            "Epoch 94/200\n",
            "859/859 [==============================] - 0s 272us/sample - loss: 0.1759 - accuracy: 0.9988 - val_loss: 0.8069 - val_accuracy: 0.6696\n",
            "Epoch 95/200\n",
            "859/859 [==============================] - 0s 312us/sample - loss: 0.1717 - accuracy: 0.9988 - val_loss: 0.8048 - val_accuracy: 0.6609\n",
            "Epoch 96/200\n",
            "859/859 [==============================] - 0s 304us/sample - loss: 0.1678 - accuracy: 0.9988 - val_loss: 0.8028 - val_accuracy: 0.6609\n",
            "Epoch 97/200\n",
            "859/859 [==============================] - 0s 297us/sample - loss: 0.1639 - accuracy: 0.9988 - val_loss: 0.8010 - val_accuracy: 0.6609\n",
            "Epoch 98/200\n",
            "859/859 [==============================] - 0s 298us/sample - loss: 0.1602 - accuracy: 0.9988 - val_loss: 0.7976 - val_accuracy: 0.6696\n",
            "Epoch 99/200\n",
            "859/859 [==============================] - 0s 302us/sample - loss: 0.1565 - accuracy: 0.9988 - val_loss: 0.7954 - val_accuracy: 0.6696\n",
            "Epoch 100/200\n",
            "859/859 [==============================] - 0s 320us/sample - loss: 0.1529 - accuracy: 0.9988 - val_loss: 0.7935 - val_accuracy: 0.6696\n",
            "Epoch 101/200\n",
            "859/859 [==============================] - 0s 278us/sample - loss: 0.1496 - accuracy: 0.9988 - val_loss: 0.7923 - val_accuracy: 0.6696\n",
            "Epoch 102/200\n",
            "859/859 [==============================] - 0s 312us/sample - loss: 0.1462 - accuracy: 0.9988 - val_loss: 0.7899 - val_accuracy: 0.6696\n",
            "Epoch 103/200\n",
            "859/859 [==============================] - 0s 308us/sample - loss: 0.1430 - accuracy: 0.9988 - val_loss: 0.7889 - val_accuracy: 0.6696\n",
            "Epoch 104/200\n",
            "859/859 [==============================] - 0s 294us/sample - loss: 0.1399 - accuracy: 0.9988 - val_loss: 0.7867 - val_accuracy: 0.6696\n",
            "Epoch 105/200\n",
            "859/859 [==============================] - 0s 297us/sample - loss: 0.1370 - accuracy: 0.9988 - val_loss: 0.7851 - val_accuracy: 0.6696\n",
            "Epoch 106/200\n",
            "859/859 [==============================] - 0s 310us/sample - loss: 0.1343 - accuracy: 0.9988 - val_loss: 0.7839 - val_accuracy: 0.6696\n",
            "Epoch 107/200\n",
            "859/859 [==============================] - 0s 317us/sample - loss: 0.1316 - accuracy: 0.9988 - val_loss: 0.7820 - val_accuracy: 0.6696\n",
            "Epoch 108/200\n",
            "859/859 [==============================] - 0s 295us/sample - loss: 0.1290 - accuracy: 0.9988 - val_loss: 0.7802 - val_accuracy: 0.6696\n",
            "Epoch 109/200\n",
            "859/859 [==============================] - 0s 285us/sample - loss: 0.1266 - accuracy: 0.9988 - val_loss: 0.7794 - val_accuracy: 0.6696\n",
            "Epoch 110/200\n",
            "859/859 [==============================] - 0s 291us/sample - loss: 0.1242 - accuracy: 0.9988 - val_loss: 0.7784 - val_accuracy: 0.6696\n",
            "Epoch 111/200\n",
            "859/859 [==============================] - 0s 294us/sample - loss: 0.1219 - accuracy: 0.9988 - val_loss: 0.7767 - val_accuracy: 0.6696\n",
            "Epoch 112/200\n",
            "859/859 [==============================] - 0s 307us/sample - loss: 0.1197 - accuracy: 0.9988 - val_loss: 0.7750 - val_accuracy: 0.6696\n",
            "Epoch 113/200\n",
            "859/859 [==============================] - 0s 290us/sample - loss: 0.1176 - accuracy: 0.9988 - val_loss: 0.7744 - val_accuracy: 0.6696\n",
            "Epoch 114/200\n",
            "859/859 [==============================] - 0s 274us/sample - loss: 0.1155 - accuracy: 0.9988 - val_loss: 0.7737 - val_accuracy: 0.6783\n",
            "Epoch 115/200\n",
            "859/859 [==============================] - 0s 310us/sample - loss: 0.1135 - accuracy: 0.9988 - val_loss: 0.7721 - val_accuracy: 0.6783\n",
            "Epoch 116/200\n",
            "859/859 [==============================] - 0s 314us/sample - loss: 0.1117 - accuracy: 0.9988 - val_loss: 0.7719 - val_accuracy: 0.6696\n",
            "Epoch 117/200\n",
            "859/859 [==============================] - 0s 272us/sample - loss: 0.1098 - accuracy: 0.9988 - val_loss: 0.7704 - val_accuracy: 0.6783\n",
            "Epoch 118/200\n",
            " 32/859 [>.............................] - ETA: 0s - loss: 0.1050 - accuracy: 1.0000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-5dbbc86dec26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_onecycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mfit_onecycle\u001b[0;34m(self, lr, epochs, checkpoint_folder, cycle_momentum, verbose, class_weight, callbacks)\u001b[0m\n\u001b[1;32m    708\u001b[0m         hist = self.fit(lr, epochs, early_stopping=None,\n\u001b[1;32m    709\u001b[0m                         \u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m                         verbose=verbose, class_weight=class_weight, callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m    711\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lr, n_cycles, cycle_len, cycle_mult, lr_decay, checkpoint_folder, early_stopping, verbose, class_weight, callbacks)\u001b[0m\n\u001b[1;32m    967\u001b[0m                                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                                   \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                                   callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msgdr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01o6KCQv3Fpn",
        "colab_type": "code",
        "outputId": "6c756b2a-fd84-47e9-fe95-d6ac9cf686af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "learner.validate(class_names=transformer.get_classes())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DRAM       0.00      0.00      0.00        89\n",
            "        HUMA       0.09      0.09      0.09        97\n",
            "        LEGA       0.05      0.03      0.04        97\n",
            "        NARR       0.00      0.00      0.00        97\n",
            "        NEWS       0.50      0.00      0.01       286\n",
            "     NEWS-P4       0.00      1.00      0.00         1\n",
            "        SCIE       0.12      0.11      0.12        97\n",
            "        SERM       0.10      0.02      0.03        95\n",
            "\n",
            "    accuracy                           0.03       859\n",
            "   macro avg       0.11      0.16      0.04       859\n",
            "weighted avg       0.21      0.03      0.03       859\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   6,   2,   0,   0,  68,  10,   3],\n",
              "       [  0,   9,   4,   0,   0,  72,  12,   0],\n",
              "       [  2,  14,   3,   0,   0,  56,  22,   0],\n",
              "       [  1,  17,  12,   0,   0,  53,  12,   2],\n",
              "       [  2,  27,  33,   0,   1, 190,  19,  14],\n",
              "       [  0,   0,   0,   0,   0,   1,   0,   0],\n",
              "       [  0,   5,   4,   0,   1,  76,  11,   0],\n",
              "       [  7,  19,   8,   0,   0,  53,   6,   2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxBPDXkv4sH0",
        "colab_type": "text"
      },
      "source": [
        "## 4. standard gru"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ7B5Ox_4PF0",
        "colab_type": "code",
        "outputId": "077c9005-ad63-4195-e863-6e5cf16f4b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "model = text.text_classifier('standard_gru', (x_train, y_train), preproc=preproc)\n",
        "\n",
        "learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1cd34136ddf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'standard_gru'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreproc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mktrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/text/models.py\u001b[0m in \u001b[0;36mtext_classifier\u001b[0;34m(name, train_data, preproc, multilabel, verbose)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mis_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_data_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_bert\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mBERT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mis_bert\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBERT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"if '%s' is selected model, then preprocess_mode='%s' should be used and vice versa\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBERT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBERT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mis_huggingface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_huggingface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_huggingface\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHUGGINGFACE_MODELS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mis_huggingface\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHUGGINGFACE_MODELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: if 'bert' is selected model, then preprocess_mode='bert' should be used and vice versa"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g099WECg4122",
        "colab_type": "code",
        "outputId": "eddc4209-83ed-4d96-bd30-6ab98e2d7719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "learner.lr_find()\n",
        "learner.lr_plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "simulating training for different learning rates... this may take a few moments...\n",
            "Train on 115 samples\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ktrain/core.py:442: UserWarning: max_epochs is being set to 5 since steps per epoch is small. If you wish to estimate LR using more epochs, set max_epochs manually.\n",
            "  'If you wish to estimate LR using more epochs, set max_epochs manually.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "115/115 [==============================] - 3s 22ms/sample - loss: 2.0838 - accuracy: 0.1217\n",
            "Epoch 2/5\n",
            "115/115 [==============================] - 0s 4ms/sample - loss: 2.0924 - accuracy: 0.0870\n",
            "Epoch 3/5\n",
            "115/115 [==============================] - 0s 4ms/sample - loss: 2.0864 - accuracy: 0.1130\n",
            "Epoch 4/5\n",
            "115/115 [==============================] - 0s 4ms/sample - loss: 2.3858 - accuracy: 0.2348\n",
            "Epoch 5/5\n",
            "115/115 [==============================] - 0s 3ms/sample - loss: nan - accuracy: 0.0870\n",
            "\n",
            "\n",
            "done.\n",
            "Please invoke the Learner.lr_plot() method to visually inspect the loss plot to help identify the maximal learning rate associated with falling loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZGo/H8fZKvEWbwodwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNAGw2d849G_",
        "colab_type": "code",
        "outputId": "9ed0f194-c172-4676-b1c3-f172ebf103a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "learner.fit_onecycle(0.01, 200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b1fcdf277cfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_onecycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'learner' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVr22ABB5T73",
        "colab_type": "text"
      },
      "source": [
        "## Another bert model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swqOgSP85YvB",
        "colab_type": "code",
        "outputId": "1d3c943e-ad1e-4b82-e010-6b3673089122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_array(x_train=df_train.text,\n",
        "                                                                      y_train=df_train.label,\n",
        "                                                                      x_test=df_test.text,\n",
        "                                                                      y_test=df_test.label,\n",
        "                                                                      lang='de',\n",
        "                                                                      preprocess_mode='bert',\n",
        "                                                                      maxlen=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessing train...\n",
            "language: de\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "preprocessing test...\n",
            "language: de\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xXPtEk05B7X",
        "colab_type": "code",
        "outputId": "2b5dd071-7015-4f96-e881-2e2407f5654f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model = text.text_classifier('bert', (x_train, y_train), preproc=preproc)\n",
        "learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 200\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTU0fHxn5jZr",
        "colab_type": "code",
        "outputId": "29e96375-30d1-42f8-ec63-259020d72b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "learner.lr_find()\n",
        "learner.lr_plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "simulating training for different learning rates... this may take a few moments...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ktrain/core.py:442: UserWarning: max_epochs is being set to 5 since steps per epoch is small. If you wish to estimate LR using more epochs, set max_epochs manually.\n",
            "  'If you wish to estimate LR using more epochs, set max_epochs manually.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 115 samples\n",
            "Epoch 1/5\n",
            "115/115 [==============================] - 28s 242ms/sample - loss: 2.1748 - accuracy: 0.1304\n",
            "Epoch 2/5\n",
            "115/115 [==============================] - 8s 68ms/sample - loss: 2.0787 - accuracy: 0.1304\n",
            "Epoch 3/5\n",
            "115/115 [==============================] - 8s 68ms/sample - loss: 2.2600 - accuracy: 0.1478\n",
            "Epoch 4/5\n",
            "115/115 [==============================] - 8s 69ms/sample - loss: 7.7719 - accuracy: 0.1304\n",
            "Epoch 5/5\n",
            " 64/115 [===============>..............] - ETA: 3s - loss: 40.0469 - accuracy: 0.1406\n",
            "\n",
            "done.\n",
            "Please invoke the Learner.lr_plot() method to visually inspect the loss plot to help identify the maximal learning rate associated with falling loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3eBcCja5NZh",
        "colab_type": "code",
        "outputId": "9ee8dee7-4e23-40a7-f0ea-b23b34558331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        }
      },
      "source": [
        "learner.autofit(2e-5, checkpoint_folder='/tmp/saved_weights')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "early_stopping automatically enabled at patience=5\n",
            "reduce_on_plateau automatically enabled at patience=2\n",
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 2e-05...\n",
            "Train on 115 samples, validate on 859 samples\n",
            "Epoch 1/1024\n",
            "115/115 [==============================] - 27s 232ms/sample - loss: 2.1591 - accuracy: 0.1652 - val_loss: 2.1747 - val_accuracy: 0.1246\n",
            "Epoch 2/1024\n",
            "115/115 [==============================] - 25s 220ms/sample - loss: 2.0510 - accuracy: 0.1217 - val_loss: 2.0730 - val_accuracy: 0.1234\n",
            "Epoch 3/1024\n",
            "115/115 [==============================] - 25s 217ms/sample - loss: 2.0029 - accuracy: 0.1304 - val_loss: 2.0106 - val_accuracy: 0.1385\n",
            "Epoch 4/1024\n",
            "115/115 [==============================] - 25s 216ms/sample - loss: 1.9535 - accuracy: 0.2261 - val_loss: 1.9563 - val_accuracy: 0.2200\n",
            "Epoch 5/1024\n",
            "115/115 [==============================] - 25s 215ms/sample - loss: 1.9101 - accuracy: 0.2957 - val_loss: 1.9087 - val_accuracy: 0.3062\n",
            "Epoch 6/1024\n",
            "115/115 [==============================] - 25s 214ms/sample - loss: 1.8485 - accuracy: 0.3130 - val_loss: 1.8720 - val_accuracy: 0.3329\n",
            "Epoch 7/1024\n",
            "115/115 [==============================] - 25s 215ms/sample - loss: 1.7719 - accuracy: 0.4087 - val_loss: 1.8558 - val_accuracy: 0.3411\n",
            "Epoch 8/1024\n",
            "115/115 [==============================] - 25s 213ms/sample - loss: 1.7160 - accuracy: 0.4000 - val_loss: 1.8244 - val_accuracy: 0.3376\n",
            "Epoch 9/1024\n",
            "115/115 [==============================] - 24s 212ms/sample - loss: 1.6517 - accuracy: 0.4348 - val_loss: 1.8073 - val_accuracy: 0.3423\n",
            "Epoch 10/1024\n",
            "115/115 [==============================] - 25s 215ms/sample - loss: 1.5904 - accuracy: 0.4522 - val_loss: 1.7995 - val_accuracy: 0.3539\n",
            "Epoch 11/1024\n",
            "115/115 [==============================] - 24s 213ms/sample - loss: 1.5049 - accuracy: 0.5130 - val_loss: 1.7862 - val_accuracy: 0.3667\n",
            "Epoch 12/1024\n",
            "115/115 [==============================] - 25s 216ms/sample - loss: 1.4099 - accuracy: 0.5913 - val_loss: 1.7729 - val_accuracy: 0.3586\n",
            "Epoch 13/1024\n",
            "115/115 [==============================] - 25s 215ms/sample - loss: 1.3533 - accuracy: 0.5826 - val_loss: 1.7604 - val_accuracy: 0.3679\n",
            "Epoch 14/1024\n",
            "115/115 [==============================] - 24s 213ms/sample - loss: 1.2763 - accuracy: 0.6522 - val_loss: 1.7451 - val_accuracy: 0.3830\n",
            "Epoch 15/1024\n",
            "115/115 [==============================] - 25s 214ms/sample - loss: 1.2182 - accuracy: 0.6000 - val_loss: 1.7192 - val_accuracy: 0.3772\n",
            "Epoch 16/1024\n",
            "115/115 [==============================] - 25s 214ms/sample - loss: 1.1211 - accuracy: 0.7130 - val_loss: 1.7115 - val_accuracy: 0.3783\n",
            "Epoch 17/1024\n",
            "115/115 [==============================] - 23s 201ms/sample - loss: 1.0421 - accuracy: 0.7391 - val_loss: 1.7125 - val_accuracy: 0.3760\n",
            "Epoch 18/1024\n",
            " 96/115 [========================>.....] - ETA: 1s - loss: 1.0003 - accuracy: 0.8229\n",
            "Epoch 00018: Reducing Max LR on Plateau: new max lr will be 1e-05 (if not early_stopping).\n",
            "115/115 [==============================] - 23s 201ms/sample - loss: 0.9870 - accuracy: 0.8174 - val_loss: 1.7156 - val_accuracy: 0.3853\n",
            "Epoch 19/1024\n",
            "115/115 [==============================] - 23s 200ms/sample - loss: 0.9216 - accuracy: 0.7391 - val_loss: 1.7230 - val_accuracy: 0.3900\n",
            "Epoch 20/1024\n",
            " 96/115 [========================>.....] - ETA: 1s - loss: 0.9715 - accuracy: 0.7396\n",
            "Epoch 00020: Reducing Max LR on Plateau: new max lr will be 5e-06 (if not early_stopping).\n",
            "115/115 [==============================] - 23s 200ms/sample - loss: 0.9477 - accuracy: 0.7565 - val_loss: 1.7186 - val_accuracy: 0.4016\n",
            "Epoch 21/1024\n",
            " 96/115 [========================>.....] - ETA: 1s - loss: 0.8760 - accuracy: 0.7500Restoring model weights from the end of the best epoch.\n",
            "115/115 [==============================] - 24s 207ms/sample - loss: 0.8710 - accuracy: 0.7739 - val_loss: 1.7145 - val_accuracy: 0.3981\n",
            "Epoch 00021: early stopping\n",
            "Weights from best epoch have been loaded into model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe3141935f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpSgoHmK55nP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm -rf /tmp/saved_weights"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}